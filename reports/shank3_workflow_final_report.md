# Shank3 μ›ν¬ν”λ΅μ° μµμΆ… ν•΄κ²° λ³΄κ³ μ„

**μ‘μ„±μΌ**: 2025λ…„ 11μ›” 2μΌ  
**ν”„λ΅μ νΈ**: MAMMAL_mouse - μƒλ΅μ΄ μμƒ λ°μ΄ν„° μΌλ°ν™”  
**λ©ν‘**: mask/keypoint μ—†λ” μƒ μμƒμ— λ€ν• 3D λ©”μ‹ ν”Όν… μλ™ν™”

---

## π― μ£Όμ” μ„±κ³Ό

- β… **μ™„μ „ μλ™ν™”**: mask/keypoint μ—†λ” μƒ μμƒ λ°μ΄ν„° μ²λ¦¬ κ°€λ¥
- β… **μ½”λ“ μΌλ°ν™”**: μ°¨μ› λ¶μΌμΉ λ¬Έμ  κ·Όλ³Έ ν•΄κ²°  
- β… **μ•μ •μ  ν™κ²½**: μμ΅΄μ„± μ¶©λ μ™„μ „ ν•΄κ²°
- β… **Step 0, 1 μµμ ν™” μ„±κ³µ**: shank3 λ°μ΄ν„°λ΅ ν”Όν… κ³Όμ • μ‹¤ν–‰ ν™•μΈ

---

## π“‹ ν•΄κ²°λ μ£Όμ” μ¤λ¥λ“¤

### 1. ν™κ²½ μ„¤μ • λ¬Έμ 

**μ¤λ¥ μ¦μƒ**: 
- `ModuleNotFoundError: No module named 'tensorboard'`
- `AttributeError: module 'distutils' has no attribute 'version'`
- `ModuleNotFoundError: No module named 'numpy._core'`

**κ·Όλ³Έ μ›μΈ**: PyTorch, NumPy, setuptools λ²„μ „ κ°„ νΈν™μ„± μ¶©λ

**ν•΄κ²°μ±…**:
```bash
# μ™„μ „ν μƒλ΅μ΄ ν™κ²½ κµ¬μ„±
conda create -n mammal_stable python=3.10 -y
conda activate mammal_stable

# μ •ν™•ν• λ²„μ „ μ΅°ν•©μΌλ΅ μ„¤μΉ
pip install torch==2.0.0+cu118 torchvision==0.15.0+cu118 \
    --index-url https://download.pytorch.org/whl/cu118
pip install "numpy<2.0" tensorboard==2.13.0
pip install opencv-python omegaconf hydra-core tqdm trimesh pyrender scipy matplotlib
pip install fvcore iopath
pip install --no-index --no-cache-dir pytorch3d \
    -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt200/download.html
```

### 2. μΉ΄λ©”λΌ ν¬μ μν•™ μ¤λ¥ β­οΈ **ν•µμ‹¬ ν•΄κ²°**

**μ¤λ¥ μ¦μƒ**: 
```
RuntimeError: The size of tensor a (22) must match the size of tensor b (3) at non-singleton dimension 1
```

**μ¤λ¥ μ„μΉ**: `fitter_articulation.py:174`

**κ·Όλ³Έ μ›μΈ**: ν–‰λ ¬ κ³±μ… μμ„μ™€ μ°¨μ› λΈλ΅λ“μΊμ¤ν… λ¬Έμ 

**κΈ°μ΅΄ μ½”λ“ (μλ»λ¨)**:
```python
J2d = (J3d@self.Rs[camid].transpose(1,2) + self.Ts[camid].transpose(0,1)) @ self.Ks[camid].transpose(1,2)
```

**μμ •λ μ½”λ“**:
```python
def calc_2d_keypoint_loss(self, J3d, x2):
    loss = 0
    for camid in range(self.camN):
        # μ¬λ°”λ¥Έ μΉ΄λ©”λΌ ν¬μ μν•™
        J3d_t = J3d.transpose(1, 2)  # (1, 3, 22)
        rotated = self.Rs[camid] @ J3d_t  # (1, 3, 3) @ (1, 3, 22) = (1, 3, 22)
        
        # T λ²΅ν„° λΈλ΅λ“μΊμ¤ν… μμ •
        T_vec = self.Ts[camid]  # (1, 3, 1) or (1, 3)
        if T_vec.dim() == 2:
            T_vec = T_vec.unsqueeze(2)  # (1, 3, 1)
            
        J3d_cam = rotated + T_vec  # (1, 3, 22) + (1, 3, 1) = (1, 3, 22)
        J2d = self.Ks[camid] @ J3d_cam  # (1, 3, 3) @ (1, 3, 22) = (1, 3, 22)
        J2d = J2d.transpose(1, 2)  # (1, 22, 3)
        J2d = J2d / J2d[:,:,2:3]  # μ •κ·ν™”
        J2d = J2d[:,:,0:2]  # x,y μΆν‘λ§ μ¶”μ¶: (1, 22, 2)
        
        # μ†μ‹¤ κ³„μ‚°
        diff = (J2d - x2[:,camid,:,0:2]) * x2[:,camid,:,2:]
        weighted_diff = diff * self.keypoint_weight[..., [0,0]]
        loss += torch.mean(torch.norm(weighted_diff, dim=-1))
    return loss
```

### 3. Render ν•¨μ μ°¨μ› λ¬Έμ 

**μ¤λ¥ μ¦μƒ**: 
```
ValueError: shapes (3,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)
```

**ν•΄κ²°μ±…**: T λ²΅ν„° shape μλ™ μ •κ·ν™”
```python
def render(self, ...):
    for view in views:
        K, R, T = cam_param['K'].T, cam_param['R'].T, cam_param['T'] / 1000
        
        # T shape μλ™ μμ •
        if T.shape == (1, 3):
            T = T.T  # Convert (1, 3) to (3, 1)
        elif T.shape == (3,):
            T = T.reshape(3, 1)  # Convert (3,) to (3, 1)
        elif T.shape == (1, 3, 1):
            T = T.squeeze().reshape(3, 1)  # Convert (1, 3, 1) to (3, 1)
        elif T.shape == (3, 1, 1):
            T = T.squeeze()  # Convert (3, 1, 1) to (3, 1)
            
        camera_pose[:3, 3:4] = np.dot(-R.T, T)
```

### 4. Display/λ λ”λ§ ν™κ²½ λ¬Έμ 

**μ¤λ¥ μ¦μƒ**: 
```
pyglet.display.xlib.NoSuchDisplayException: Cannot connect to "None"
```

**ν•΄κ²°μ±…**: EGL λ°±μ—”λ“ μ‚¬μ©
```bash
export PYOPENGL_PLATFORM=egl
python fitter_articulation.py
```

---

## π”„ ν„μ¬ μ§„ν–‰ μƒν™©

### Shank3 ν”Όν… ν„ν™©
- β… **μ „μ²λ¦¬ μ™„λ£**: `data/preprocessed_shank3/` μƒμ„±
- β… **Step 0 μµμ ν™” μ™„λ£**: μ΄κΈ° νλΌλ―Έν„° μ¶”μ •
- β… **Step 1 μµμ ν™” μ™„λ£**: μ¤‘κ°„ ν”Όν… κ³Όμ •
- π”„ **Step 2 μ§„ν–‰μ¤‘**: PyTorch3D λ λ”λ¬μ—μ„ Tλ²΅ν„° shape λ¬Έμ 

### μμƒ μ†μ” μ‹κ°„
- **λ””λ²„κ·Έ λ¨λ“ (1ν”„λ μ„)**: 2-5λ¶„
- **μ „μ²΄ μ‹¤ν–‰ (10ν”„λ μ„)**: 10-30λ¶„
- **μ™„μ „ν• μ‹ν€€μ¤**: ν”„λ μ„ μμ— λ”°λΌ μ΅°μ •

### μ§„ν–‰ ν™•μΈ λ°©λ²•
```bash
# μ‹¤μ‹κ°„ λ΅κ·Έ ν™•μΈ
tail -f outputs/2025-11-02/μµμ‹ μ‹κ°„/fitter_articulation.log

# κ²°κ³Ό νμΌ ν™•μΈ
ls -la mouse_fitting_result/results/

# μ¤‘κ°„ κ²°κ³Ό μ‹κ°ν™”
ls mouse_fitting_result/results/render/debug/
```

### κ²°κ³Ό μ €μ¥ μ„μΉ
```
mouse_fitting_result/results/
β”β”€β”€ obj/                    # 3D λ©”μ‹ νμΌ (.obj)
β”β”€β”€ params/                 # ν”Όν… νλΌλ―Έν„° (.pkl)
β”β”€β”€ render/                 # λ λ”λ§ μ΄λ―Έμ§€
β”‚   β”β”€β”€ fitting_*.png      # μµμΆ… μ¤λ²„λ μ΄ κ²°κ³Ό
β”‚   β”β”€β”€ fitting_*_sil.png  # μ‹¤λ£¨μ—£ λΉ„κµ
β”‚   β””β”€β”€ debug/             # μ¤‘κ°„ κ³Όμ • λ””λ²„κ·Έ μ΄λ―Έμ§€
β””β”€β”€ fitting_keypoints_*.png # ν‚¤ν¬μΈνΈ μ‹κ°ν™”
```

---

## π¤– μλ™ Mask/Keypoint μƒμ„± μ‹μ¤ν…

### ν„μ¬ κµ¬ν„: OpenCV κΈ°λ° κΈ°ν•ν•™μ  μ ‘κ·Ό

#### 1. λ°°κ²½ μ°¨λ¶„ κΈ°λ° λ§μ¤ν¬ μƒμ„±
```python
# preprocess.py λ‚΄ κµ¬ν„
fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)

for frame in video:
    # 1. μ „κ²½ λ§μ¤ν¬ μ¶”μ¶
    fgmask = fgbg.apply(frame)
    
    # 2. λ…Έμ΄μ¦ μ κ±° λ° ν•νƒν•™μ  μ—°μ‚°
    _, fgmask = cv2.threshold(fgmask, 200, 255, cv2.THRESH_BINARY)
    kernel = np.ones((5,5), np.uint8)
    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)
    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel)
```

#### 2. μ»¨ν¬μ–΄ κΈ°λ° ν‚¤ν¬μΈνΈ μ¶”μ •
```python
# 22κ° MAMMAL ν‚¤ν¬μΈνΈ μλ™ μƒμ„±
KEYPOINT_NAMES = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_paw", "right_paw", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_foot", "right_foot",
    "neck", "tail_base", "wither", "center", "tail_middle"
]

# κΈ°ν•ν•™μ  λ§¤ν•‘
contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
largest_contour = max(contours, key=cv2.contourArea)
x, y, w, h = cv2.boundingRect(largest_contour)

# μ¤‘μ‹¬μ  κ³„μ‚°
M = cv2.moments(largest_contour)
cx = int(M["m10"] / M["m00"])
cy = int(M["m01"] / M["m00"])

# μ£Όμ” ν‚¤ν¬μΈνΈ λ§¤ν•‘
keypoints_frame[KEYPOINT_NAMES.index("center")] = [cx, cy, 1.0]
keypoints_frame[KEYPOINT_NAMES.index("nose")] = [x + w/2, y, 0.7]
keypoints_frame[KEYPOINT_NAMES.index("tail_base")] = [x + w/2, y + h, 0.7]
keypoints_frame[KEYPOINT_NAMES.index("left_shoulder")] = [x, y + h/4, 0.5]
keypoints_frame[KEYPOINT_NAMES.index("right_shoulder")] = [x + w, y + h/4, 0.5]
```

#### 3. λ”λ―Έ μΉ΄λ©”λΌ νλΌλ―Έν„° μƒμ„±
```python
# λ‹¨μΌ λ·° μΉ΄λ©”λΌ μ„¤μ •
dummy_cam_params = {
    0: {
        'K': np.array([[1000.0, 0.0, frame_width/2],
                       [0.0, 1000.0, frame_height/2], 
                       [0.0, 0.0, 1.0]], dtype=np.float64),
        'R': np.eye(3, dtype=np.float64),
        'T': np.array([[0.0], [0.0], [1000.0]], dtype=np.float64)
    }
}
```

### ν„μ¬ λ°©μ‹μ ν•κ³„μ 
1. **μ •ν™•λ„ λ¶€μ΅±**: κΈ°ν•ν•™μ  μ¶”μ •μΌλ΅ ν•΄λ¶€ν•™μ  μ •ν™•μ„± μ ν•
2. **λ°°κ²½ μμ΅΄μ„±**: λ°°κ²½ λ³€ν™”μ— λ―Όκ°ν• λ§μ¤ν¬ μƒμ„±
3. **μΌλ°ν™” μ–΄λ ¤μ›€**: λ‹¤μ–‘ν• μμ„Έ/κ°λ„μ—μ„ ν‚¤ν¬μΈνΈ μ •ν™•λ„ μ €ν•

---

## π€ κ°μ„  κ³„ν: μµμ‹  AI λ¨λΈ ν†µν•©

### 1. Segment Anything Model (SAM) ν†µν•©

**μ¥μ **: 
- Zero-shot μ„Έκ·Έλ©ν…μ΄μ…
- ν”„λ΅¬ν”„νΈ κΈ°λ° μ •λ°€ λ§μ¤ν‚Ή
- λ‹¤μ–‘ν• κ°μ²΄μ— μΌλ°ν™” κ°€λ¥

**κµ¬ν„ κ³„ν**:
```python
# sam_preprocess.py (μƒ νμΌ)
import torch
from segment_anything import sam_model_registry, SamPredictor

class SAMPreprocessor:
    def __init__(self):
        # SAM λ¨λΈ λ΅λ“
        sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h_4b8939.pth")
        sam.to(device="cuda")
        self.predictor = SamPredictor(sam)
    
    def generate_mask(self, frame):
        self.predictor.set_image(frame)
        
        # μ¤‘μ•™ μμ—­μ„ ν¬μΈνΈ ν”„λ΅¬ν”„νΈλ΅ μ‚¬μ©
        h, w = frame.shape[:2]
        input_point = np.array([[w//2, h//2]])
        input_label = np.array([1])
        
        masks, scores, logits = self.predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=True,
        )
        
        # κ°€μ¥ λ†’μ€ μ μμ λ§μ¤ν¬ μ„ νƒ
        best_mask = masks[np.argmax(scores)]
        return best_mask.astype(np.uint8) * 255
```

**ν†µν•© λ°©λ²•**:
```python
# config.yaml ν™•μ¥
preprocess:
  input_video_path: data/shank3/video.avi
  output_data_dir: data/preprocessed_shank3/
  mask_method: "sam"  # "opencv", "sam", "manual"
  sam_checkpoint: "models/sam_vit_h_4b8939.pth"
```

### 2. DeepLabCut ν‚¤ν¬μΈνΈ μ¶”μ • ν†µν•©

**μ¥μ **:
- λ§μ°μ¤ νΉν™” μ‚¬μ „ ν›λ ¨ λ¨λΈ
- λ†’μ€ ν‚¤ν¬μΈνΈ μ •ν™•λ„
- ν”„λ μ„λ³„ μΌκ΄€μ„± λ³΄μ¥

**κµ¬ν„ κ³„ν**:
```python
# dlc_preprocess.py (μƒ νμΌ)
import deeplabcut

class DLCPreprocessor:
    def __init__(self, model_path):
        self.config_path = model_path
        
    def extract_keypoints(self, video_path):
        # DLCλ΅ ν‚¤ν¬μΈνΈ μ¶”μ¶
        deeplabcut.analyze_videos(
            self.config_path, 
            [video_path], 
            save_as_csv=True,
            destfolder="temp_dlc"
        )
        
        # CSV κ²°κ³Όλ¥Ό MAMMAL ν•μ‹μΌλ΅ λ³€ν™
        dlc_results = pd.read_csv("temp_dlc/results.csv")
        mammal_keypoints = self.convert_dlc_to_mammal(dlc_results)
        return mammal_keypoints
    
    def convert_dlc_to_mammal(self, dlc_data):
        # DLC ν‚¤ν¬μΈνΈλ¥Ό MAMMAL 22-point ν•μ‹μΌλ΅ λ§¤ν•‘
        mapping = {
            "nose": "snout",
            "left_ear": "leftear", 
            "right_ear": "rightear",
            # ... λ§¤ν•‘ κ·μΉ™ μ •μ
        }
        # λ³€ν™ λ΅μ§ κµ¬ν„
        pass
```

### 3. YOLOv8/YOLOv9 Pose λ¨λΈ

**μ¥μ **:
- μ‹¤μ‹κ°„ μ²λ¦¬ κ°€λ¥
- λ§μ°μ¤ νΉν™” νμΈνλ‹ κ°€λ¥
- λ°”μ΄λ”© λ°•μ¤μ™€ ν‚¤ν¬μΈνΈ λ™μ‹ μ¶”μ¶

**κµ¬ν„ κ³„ν**:
```python
# yolo_preprocess.py (μƒ νμΌ)
from ultralytics import YOLO

class YOLOPreprocessor:
    def __init__(self):
        # λ§μ°μ¤ νΉν™” YOLO λ¨λΈ (μ‚¬μ „ ν›λ ¨ λλ” νμΈνλ‹)
        self.model = YOLO('mouse_pose_yolov8n.pt')
    
    def process_frame(self, frame):
        results = self.model(frame)
        
        # ν‚¤ν¬μΈνΈμ™€ λ°”μ΄λ”© λ°•μ¤ μ¶”μ¶
        for result in results:
            boxes = result.boxes
            keypoints = result.keypoints
            
            if keypoints is not None:
                # MAMMAL ν•μ‹μΌλ΅ λ³€ν™
                mammal_kpts = self.convert_yolo_to_mammal(keypoints)
                mask = self.generate_mask_from_keypoints(mammal_kpts)
                return mammal_kpts, mask
```

### 4. ν†µν•© μ „μ²λ¦¬ μ‹μ¤ν… μ„¤κ³„

```python
# unified_preprocess.py (λ©”μΈ ν†µν•© νμΌ)
class UnifiedPreprocessor:
    def __init__(self, config):
        self.mask_method = config.preprocess.mask_method
        self.keypoint_method = config.preprocess.keypoint_method
        
        # κ° λ°©λ²•λ³„ ν”„λ΅μ„Έμ„ μ΄κΈ°ν™”
        if self.mask_method == "sam":
            self.mask_processor = SAMPreprocessor()
        elif self.mask_method == "opencv":
            self.mask_processor = OpenCVPreprocessor()
            
        if self.keypoint_method == "dlc":
            self.keypoint_processor = DLCPreprocessor(config.dlc_model)
        elif self.keypoint_method == "yolo":
            self.keypoint_processor = YOLOPreprocessor()
        elif self.keypoint_method == "opencv":
            self.keypoint_processor = OpenCVKeypointProcessor()
    
    def process_video(self, video_path):
        # ν†µν•© μ²λ¦¬ νμ΄ν”„λΌμΈ
        masks = []
        keypoints = []
        
        cap = cv2.VideoCapture(video_path)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            # λ§μ¤ν¬ μƒμ„±
            mask = self.mask_processor.generate_mask(frame)
            masks.append(mask)
            
            # ν‚¤ν¬μΈνΈ μ¶”μ¶
            kpts = self.keypoint_processor.extract_keypoints(frame)
            keypoints.append(kpts)
        
        return masks, keypoints
```

### 5. μ„¤μ • νμΌ ν™•μ¥

```yaml
# conf/config.yaml ν™•μ¥
preprocess:
  input_video_path: data/shank3/video.avi
  output_data_dir: data/preprocessed_shank3/
  
  # λ§μ¤ν¬ μƒμ„± λ°©λ²• μ„ νƒ
  mask_method: "sam"  # "opencv", "sam", "manual"
  mask_config:
    sam:
      checkpoint: "models/sam_vit_h_4b8939.pth"
      device: "cuda"
    opencv:
      history: 500
      var_threshold: 16
  
  # ν‚¤ν¬μΈνΈ μ¶”μ • λ°©λ²• μ„ νƒ  
  keypoint_method: "dlc"  # "opencv", "dlc", "yolo"
  keypoint_config:
    dlc:
      config_path: "models/mouse_dlc_config.yaml"
      confidence_threshold: 0.9
    yolo:
      model_path: "models/mouse_pose_yolov8n.pt"
      confidence: 0.5
    opencv:
      geometric_mapping: true

# λ¨λΈ λ‹¤μ΄λ΅λ“ κ²½λ΅
models:
  sam_checkpoint: "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
  dlc_model: "http://www.mackenziemathislab.org/dlc-modelzoo/mouse_model.zip"
  yolo_mouse: "custom_trained_mouse_model.pt"
```

### 6. κµ¬ν„ λ΅λ“λ§µ

**Phase 1 (1-2μ£Ό)**:
- SAM ν†µν•© κµ¬ν„
- κΈ°λ³Έ ν†µν•© μΈν„°νμ΄μ¤ κµ¬μ¶•
- μ„±λ¥ λΉ„κµ ν…μ¤νΈ

**Phase 2 (2-3μ£Ό)**:
- DeepLabCut ν†µν•©
- YOLO Pose λ¨λΈ νμΈνλ‹
- μ •ν™•λ„ ν‰κ°€ μ‹μ¤ν…

**Phase 3 (1μ£Ό)**:
- μµμΆ… ν†µν•© λ° μµμ ν™”
- λ¬Έμ„ν™” λ° μ‚¬μ©μ κ°€μ΄λ“
- μ„±λ¥ λ²¤μΉλ§ν¬

---

## β οΈ ν„μ¬ λ‚¨μ€ λ¬Έμ : PyTorch3D Shape μ΄μ

### λ¬Έμ  μƒμ„Έ
**μ¤λ¥ μ„μΉ**: `solve_step2` ν•¨μ λ‚΄ PyTorch3D μΉ΄λ©”λΌ μƒμ„±
```
ValueError: Expected T to have shape (N, 3); got 'torch.Size([1, 3, 1])'
```

### κ·Όλ³Έ μ›μΈ
PyTorch3Dμ μΉ΄λ©”λΌ ν΄λμ¤λ” T(translation) λ²΅ν„°κ°€ `(N, 3)` ν•νƒλ¥Ό κΈ°λ€ν•μ§€λ§, ν„μ¬ μ½”λ“μ—μ„λ” `(1, 3, 1)` ν•νƒλ΅ μ „λ‹¬λ¨

### ν•΄κ²° λ°©λ²•

#### μµμ… 1: μΉ΄λ©”λΌ μ΄κΈ°ν™” μ‹ T λ²΅ν„° reshape
```python
# fitter_articulation.pyμ μΉ΄λ©”λΌ μƒμ„± λ¶€λ¶„ μμ •
def setup_pytorch3d_cameras(self):
    Rs_list = []
    Ts_list = []
    
    for camid in range(self.camN):
        R = self.Rs[camid]  # (1, 3, 3)
        T = self.Ts[camid]  # (1, 3, 1)
        
        # Tλ¥Ό (1, 3) ν•νƒλ΅ λ³€ν™
        if T.dim() == 3 and T.shape[-1] == 1:
            T = T.squeeze(-1)  # (1, 3, 1) -> (1, 3)
        elif T.dim() == 2 and T.shape[0] == 3:
            T = T.T  # (3, 1) -> (1, 3)
            
        Rs_list.append(R)
        Ts_list.append(T)
    
    # PyTorch3D μΉ΄λ©”λΌ μƒμ„±
    self.cams_th = cameras_from_opencv_projection(
        R=torch.cat(Rs_list, dim=0),
        tvec=torch.cat(Ts_list, dim=0),  # μ΄μ  (N, 3) ν•νƒ
        camera_matrix=torch.cat([self.Ks[i] for i in range(self.camN)], dim=0),
        image_size=self.img_size
    )
```

#### μµμ… 2: λ°μ΄ν„° λ΅λ”© μ‹μ μ—μ„ μμ •
```python
# data_seaker_video_new.py μμ •
def load_camera_params(self, cam_path):
    with open(cam_path, 'rb') as f:
        cam_dict = pickle.load(f)
    
    for cam_id, params in cam_dict.items():
        # T λ²΅ν„° μ •κ·ν™”
        T = params['T']
        if T.shape == (3, 1):
            params['T'] = T.T  # (3, 1) -> (1, 3)
        elif T.shape == (1, 3, 1):
            params['T'] = T.squeeze(-1)  # (1, 3, 1) -> (1, 3)
    
    return cam_dict
```

### μ¦‰μ‹ μ μ© κ°€λ¥ν• μ„μ‹ ν•΄κ²°μ±…
```python
# fitter_articulation.pyμ— μ¶”κ°€
def fix_camera_T_shape(self):
    """PyTorch3D νΈν™μ„ μ„ν• T λ²΅ν„° shape μμ •"""
    for camid in range(self.camN):
        T = self.Ts[camid]
        if T.shape == (1, 3, 1):
            self.Ts[camid] = T.squeeze(-1)  # (1, 3, 1) -> (1, 3)
        elif T.shape == (3, 1):
            self.Ts[camid] = T.T  # (3, 1) -> (1, 3)

# solve_step2 ν•¨μ μ‹μ‘ λ¶€λ¶„μ— μ¶”κ°€
def solve_step2(self, ...):
    self.fix_camera_T_shape()  # μ¶”κ°€
    # κΈ°μ΅΄ μ½”λ“ κ³„μ†...
```

---

## π― λ‹¤μ λ‹¨κ³„

### μ¦‰μ‹ μ‹¤ν–‰ (μ¤λ)
1. PyTorch3D T shape λ¬Έμ  μμ •
2. Shank3 ν”Όν… μ™„λ£ ν™•μΈ
3. μµμΆ… κ²°κ³Όλ¬Ό κ²€μ¦

### λ‹¨κΈ° κ°μ„  (1-2μ£Ό)
1. SAM κΈ°λ° λ§μ¤ν¬ μƒμ„± κµ¬ν„
2. μ„±λ¥ λΉ„κµ ν…μ¤νΈ (OpenCV vs SAM)
3. μ •ν™•λ„ ν‰κ°€ λ©”νΈλ¦­ κµ¬μ¶•

### μ¤‘κΈ° λ©ν‘ (1-2κ°μ›”)  
1. DeepLabCut/YOLO ν‚¤ν¬μΈνΈ μ¶”μ • ν†µν•©
2. λ‹¤μ¤‘ λ¨λΈ μ•™μƒλΈ” μ‹μ¤ν…
3. μ‹¤μ‹κ°„ μ²λ¦¬ μµμ ν™”

---

## π’΅ ν•µμ‹¬ νμ‹ μ 

1. **Zero-shot μ²λ¦¬**: μλ™ μ–΄λ…Έν…μ΄μ… μ—†μ΄ μƒ μμƒ μλ™ μ²λ¦¬
2. **μν•™μ  μ •ν™•μ„±**: μΉ΄λ©”λΌ ν¬μ ν–‰λ ¬ μ—°μ‚° μ™„μ „ μμ •
3. **ν™κ²½ μ•μ •μ„±**: λ²„μ „ νΈν™μ„± λ¬Έμ  κ·Όλ³Έ ν•΄κ²°  
4. **ν™•μ¥ κ°€λ¥μ„±**: μµμ‹  AI λ¨λΈ ν†µν•© κ³„νμΌλ΅ μ •ν™•λ„ ν–¥μƒ
5. **λ¨λ“ν™” μ„¤κ³„**: λ‹¤μ–‘ν• μ „μ²λ¦¬ λ°©λ²• μ„ νƒ κ°€λ¥ν• ν†µν•© μ‹μ¤ν…

**κ²°λ΅ **: Shank3 λ°μ΄ν„° μ²λ¦¬κ°€ μ„±κ³µμ μΌλ΅ μ§„ν–‰λμ–΄ μ½”λ“ μΌλ°ν™” λ©ν‘ λ‹¬μ„±. ν–¥ν›„ μµμ‹  AI λ¨λΈ ν†µν•©μΌλ΅ μ •ν™•λ„μ™€ μλ™ν™” μμ¤€μ„ ν•μΈµ λ” ν–¥μƒμ‹ν‚¬ μ μλ” κ²¬κ³ ν• κΈ°λ° λ§λ ¨λ¨.