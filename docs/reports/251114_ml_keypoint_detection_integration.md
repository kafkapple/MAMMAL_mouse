# ì—°êµ¬ ë³´ê³ ì„œ: ML-based Keypoint Detection í†µí•©

**ë‚ ì§œ**: 2025-11-14
**ì£¼ì œ**: YOLOv8-Pose ë° SuperAnimalì„ í™œìš©í•œ MAMMAL mouse keypoint detection ê°œì„ 
**ì €ì**: Research Session with Claude
**ìƒíƒœ**: ğŸš§ In Progress (Phase 1 ì™„ë£Œ, Phase 2 ì§„í–‰ ì¤‘)

---

## Executive Summary

### ëª©ì 
Monocular MAMMAL fittingì˜ í•µì‹¬ bottleneckì¸ **keypoint detection í’ˆì§ˆ**ì„ ML ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ ê°œì„ .

### í•µì‹¬ ì„±ê³¼
1. âœ… **YOLOv8-Pose í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•** ì™„ë£Œ
2. âœ… **DANNCE â†’ YOLO ë³€í™˜ infrastructure** êµ¬ì¶•
3. âœ… **SuperAnimal-TopViewMouse ëª¨ë¸** ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„ ì™„ë£Œ
4. ğŸš§ **SuperAnimal í†µí•©** ì§„í–‰ ì¤‘

### ì£¼ìš” ì¸ì‚¬ì´íŠ¸
- **Garbage in, garbage out**: Geometric keypointsë¡œ í•™ìŠµ ì‹œ ì˜ë¯¸ ì—†ëŠ” ê²°ê³¼
- **Pretrained models í™œìš© í•„ìˆ˜**: SuperAnimal (27 kpts) â†’ MAMMAL (22 kpts) ë§¤í•‘ í•„ìš”
- **ê³ í’ˆì§ˆ ë¼ë²¨ í•„ìš”ì„±**: ìˆ˜ë™ ë¼ë²¨ë§ ë˜ëŠ” pretrained modelì´ critical

---

## 1. ë°°ê²½ ë° ë™ê¸°

### 1.1 ë¬¸ì œ ì •ì˜

**Monocular MAMMAL Fittingì˜ í˜„ì¬ í•œê³„** (2025-11-14 PoC ê²°ê³¼):
- Geometric PCA ê¸°ë°˜ keypoint estimation
- Confidence: ~0.40-0.60 (paws), ~0.70 (spine/head)
- Final optimization loss: ~300K (ë§¤ìš° ë†’ìŒ)
- Pose accuracy: T-pose bias (regularization ì§€ë°°)

**ìš”êµ¬ì‚¬í•­**:
- Input: Monocular RGB image
- Output: 22 MAMMAL keypoints with high confidence (>0.90)
- Expected: 10-20Ã— lower loss (~15K-30K)

### 1.2 ì ‘ê·¼ ë°©ë²•

**Option A: YOLOv8-Pose Fine-tuning** (ë¹ ë¥¸ êµ¬í˜„)
- Pros: Fast training, lightweight, real-time inference
- Cons: Requires quality labels

**Option B: SuperAnimal-TopViewMouse** (ê³ í’ˆì§ˆ)
- Pros: Pretrained on 5K+ mice, proven accuracy
- Cons: 27 keypoints â†’ 22 mapping needed, DLC dependency

**Decision**: **Both approaches** (ë³‘ë ¬ ê°œë°œ)
- Phase 1: YOLOv8 infrastructure (ì™„ë£Œ)
- Phase 2: SuperAnimal integration (ì§„í–‰ ì¤‘)

---

## 2. Phase 1: YOLOv8-Pose í†µí•©

### 2.1 Dataset Conversion (DANNCE â†’ YOLO)

**êµ¬í˜„**: `preprocessing_utils/dannce_to_yolo.py`

**YOLO Pose Label Format**:
```
<class_id> <x_center> <y_center> <width> <height> <kpt1_x> <kpt1_y> <kpt1_v> ... <kpt22_x> <kpt22_y> <kpt22_v>
```

**Key Features**:
- BBox clipping to image bounds (ì¤‘ìš”!)
- Keypoint confidence â†’ visibility mapping
- Flip augmentation indices for left/right symmetry

**ë³€í™˜ ê²°ê³¼**:
```
Dataset: 50 train, 10 val images
Time: ~30 seconds for full conversion
Output: data/yolo_mouse_pose/
  â”œâ”€â”€ images/train/  (50 images)
  â”œâ”€â”€ labels/train/  (50 labels)
  â”œâ”€â”€ images/val/    (10 images)
  â”œâ”€â”€ labels/val/    (10 labels)
  â””â”€â”€ data.yaml      (config)
```

**data.yaml Configuration**:
```yaml
nc: 1  # Single class: mouse
kpt_shape: [22, 3]  # 22 keypoints, (x, y, visibility)
flip_idx: [0, 2, 1, 4, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 14, 17, 16, 18, 19, 20, 21]
```

### 2.2 YOLOv8-Pose Training

**í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸**: `train_yolo_pose.py`

**Configuration**:
```python
Model: yolov8n-pose (3.4M parameters)
Epochs: 10 (test run)
Batch size: 4
Image size: 256Ã—256
Device: CUDA (RTX 3060 12GB)
Optimizer: Adam (lr=0.001)
Augmentation: Light (flipl r=0.5, rotation=10Â°, scale=0.2)
```

**í•™ìŠµ ê²°ê³¼** (10 epochs, ~15 minutes):
```
âœ… Training completed successfully
ğŸ“Š Metrics:
   Box mAP50: 0.0012
   Box mAP50-95: 0.0004
   Pose mAP50: 0.0000
   Pose mAP50-95: 0.0000

âš ï¸ Near-zero performance (ì˜ˆìƒë¨)
```

**ì‹¤íŒ¨ ì›ì¸ ë¶„ì„**:
1. **Training data quality**: Geometric keypoints (ë‚®ì€ ì •í™•ë„)
2. **Label noise**: Confidence ~0.40-0.60 â†’ unreliable supervision
3. **Small dataset**: 50 images (ì¼ë°˜ì ìœ¼ë¡œ 1K+ í•„ìš”)

**êµí›ˆ**:
- **Garbage in, garbage out**: ML ëª¨ë¸ì€ ë°ì´í„° í’ˆì§ˆì— ì ˆëŒ€ì ìœ¼ë¡œ ì˜ì¡´
- **Pretrained models í•„ìˆ˜**: Transfer learning ì—†ì´ scratch trainingì€ ë¹„í˜„ì‹¤ì 

### 2.3 YOLOv8KeypointDetector êµ¬í˜„

**êµ¬í˜„**: `preprocessing_utils/yolo_keypoint_detector.py`

**Key Features**:
```python
class YOLOv8KeypointDetector:
    - detect(): Single image inference
    - detect_batch(): Batch inference
    - visualize(): Keypoint visualization
    - 26 keypoints â†’ 22 MAMMAL mapping (for future use)
```

**Usage**:
```python
detector = YOLOv8KeypointDetector('model.pt', device='cuda')
keypoints = detector.detect(rgb_image)  # (22, 3)
```

**í˜„ì¬ ìƒíƒœ**: âœ… êµ¬í˜„ ì™„ë£Œ, âš ï¸ ëª¨ë¸ í’ˆì§ˆ ë‚®ìŒ (ì¬í•™ìŠµ í•„ìš”)

---

## 3. Phase 2: SuperAnimal-TopViewMouse í†µí•©

### 3.1 SuperAnimal ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

**Model**: `mwmathis/DeepLabCutModelZoo-SuperAnimal-TopViewMouse`
**Source**: HuggingFace Model Hub
**Size**: 245 MB (TensorFlow checkpoint)

**ë‹¤ìš´ë¡œë“œ ê²°ê³¼**:
```
âœ… Downloaded successfully
Location: models/superanimal_topviewmouse/
Files:
  - snapshot-200000.pb (96 MB) - TensorFlow graph
  - snapshot-200000.data-00000-of-00001 (145 MB) - Weights
  - pose_cfg.yaml (config)
```

**ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸**: `download_superanimal.py`
```python
from dlclibrary import download_huggingface_model
download_huggingface_model("superanimal_topviewmouse", model_dir)
```

### 3.2 SuperAnimal Keypoint êµ¬ì¡° ë¶„ì„

**SuperAnimal keypoints (27ê°œ)**:
```python
0: nose
1: left_ear
2: right_ear
3: left_ear_tip
4: right_ear_tip
5: left_eye
6: right_eye
7: neck
8: mid_back
9: mouse_center
10: mid_backend
11: mid_backend2
12: mid_backend3
13: tail_base
14: tail1
15: tail2
16: tail3
17: tail4
18: tail5
19: left_shoulder
20: left_midside
21: left_hip
22: right_shoulder
23: right_midside
24: right_hip
25: tail_end
26: head_midpoint
```

**MAMMAL keypoints (22ê°œ)**:
```python
0: nose
1: left_ear
2: right_ear
3: left_eye
4: right_eye
5: head_center
6-13: spine_1 to spine_8 (8 points)
14: left_front_paw
15: right_front_paw
16: left_rear_paw
17: right_rear_paw
18: tail_base
19: tail_mid
20: tail_tip
21: centroid
```

### 3.3 SuperAnimal â†’ MAMMAL Mapping

**ì§ì ‘ ë§¤í•‘ (1:1)**:
```python
# Head region (exact matches)
MAMMAL[0] = SuperAnimal[0]  # nose
MAMMAL[1] = SuperAnimal[1]  # left_ear
MAMMAL[2] = SuperAnimal[2]  # right_ear
MAMMAL[3] = SuperAnimal[5]  # left_eye
MAMMAL[4] = SuperAnimal[6]  # right_eye
MAMMAL[5] = SuperAnimal[26] # head_center (head_midpoint)

# Tail region
MAMMAL[18] = SuperAnimal[13]  # tail_base
MAMMAL[20] = SuperAnimal[25]  # tail_tip
```

**ë³´ê°„ ë§¤í•‘ (interpolation)**:
```python
# Spine: SuperAnimal 4ê°œ â†’ MAMMAL 8ê°œ
# SuperAnimal: neck(7), mid_back(8), mid_backend(10), mid_backend2(11), mid_backend3(12)
# MAMMAL: spine_1 to spine_8
# Strategy: Linear interpolation along backbone

spine_sa = [7, 8, 10, 11, 12]  # 5 points
spine_mammal = interpolate_keypoints(spine_sa, n_target=8)

# Tail: SuperAnimal 6ê°œ â†’ MAMMAL 3ê°œ
# SuperAnimal: tail_base(13), tail1-5(14-18), tail_end(25)
# MAMMAL: tail_base(18), tail_mid(19), tail_tip(20)
MAMMAL[19] = interpolate([13, 14, 15, 16, 17, 18], position=0.5)
```

**ì¶”ì • ë§¤í•‘ (limbs - ê°€ì¥ challenging)**:
```python
# SuperAnimal: shoulder/midside/hip (body sides)
# MAMMAL: paw positions (extremities)

# Front paws: shoulder ê¸°ì¤€ìœ¼ë¡œ perpendicular ë°©í–¥ ì¶”ì •
MAMMAL[14] = estimate_paw_from_shoulder(SuperAnimal[19], direction='left')
MAMMAL[15] = estimate_paw_from_shoulder(SuperAnimal[22], direction='right')

# Rear paws: hip ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •
MAMMAL[16] = estimate_paw_from_hip(SuperAnimal[21], direction='left')
MAMMAL[17] = estimate_paw_from_hip(SuperAnimal[24], direction='right')
```

**Centroid (ê³„ì‚°)**:
```python
MAMMAL[21] = mean([SuperAnimal[9], all_valid_keypoints])  # mouse_center + average
```

### 3.4 SuperAnimal Inference Pipeline (ê³„íš)

**êµ¬í˜„ ì˜ˆì •**: `preprocessing_utils/superanimal_detector.py`

```python
class SuperAnimalDetector:
    def __init__(self, model_path, device='cuda'):
        # Load TensorFlow model via DeepLabCut API
        import deeplabcut
        self.model = deeplabcut.load_model(model_path)
        self.mapper = SuperAnimalToMAMMALMapper()

    def detect(self, rgb_image):
        # Run DLC inference
        sa_keypoints = self.model.predict(rgb_image)  # (27, 3)

        # Map to MAMMAL
        mammal_keypoints = self.mapper.map(sa_keypoints)  # (22, 3)

        return mammal_keypoints
```

**Dependencies**:
- `deeplabcut` (TensorFlow backend)
- `tensorflow` (GPU support)

**Challenge**: TensorFlow vs PyTorch environment compatibility

---

## 4. êµ¬í˜„ ìƒì„¸

### 4.1 ì½”ë“œ êµ¬ì¡°

```
MAMMAL_mouse/
â”œâ”€â”€ preprocessing_utils/
â”‚   â”œâ”€â”€ keypoint_estimation.py          # Geometric (baseline)
â”‚   â”œâ”€â”€ yolo_keypoint_detector.py       # YOLOv8-Pose âœ…
â”‚   â”œâ”€â”€ superanimal_detector.py         # SuperAnimal ğŸš§
â”‚   â”œâ”€â”€ dannce_to_yolo.py              # Dataset converter âœ…
â”‚   â””â”€â”€ keypoint_detector_factory.py    # Unified interface (TODO)
â”œâ”€â”€ train_yolo_pose.py                  # YOLO training âœ…
â”œâ”€â”€ download_superanimal.py             # Model download âœ…
â”œâ”€â”€ fit_monocular.py                    # Main pipeline (update TODO)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ yolo_mouse_pose/               # YOLO dataset âœ…
â””â”€â”€ models/
    â””â”€â”€ superanimal_topviewmouse/      # SuperAnimal model âœ…
```

### 4.2 Detector Factory Pattern (ê³„íš)

**ëª©í‘œ**: Unified interface for all detectors

```python
# preprocessing_utils/keypoint_detector_factory.py

class KeypointDetectorFactory:
    @staticmethod
    def create(detector_type='geometric', **kwargs):
        if detector_type == 'geometric':
            return GeometricDetector()
        elif detector_type == 'yolo':
            return YOLOv8KeypointDetector(kwargs['model_path'])
        elif detector_type == 'superanimal':
            return SuperAnimalDetector(kwargs['model_path'])
        else:
            raise ValueError(f"Unknown detector: {detector_type}")

# Usage in fit_monocular.py
detector = KeypointDetectorFactory.create('superanimal',
                                          model_path='models/superanimal_topviewmouse')
keypoints = detector.detect(rgb_image)
```

### 4.3 fit_monocular.py í†µí•© (ê³„íš)

```python
# Add CLI argument
parser.add_argument('--detector', type=str,
                    choices=['geometric', 'yolo', 'superanimal'],
                    default='geometric',
                    help='Keypoint detection method')
parser.add_argument('--detector-model', type=str,
                    help='Path to detector model (for yolo/superanimal)')

# Initialize detector
detector = KeypointDetectorFactory.create(
    args.detector,
    model_path=args.detector_model
)

# Use in fitting loop
keypoints = detector.detect(rgb_image)  # Unified interface
```

---

## 5. ì‹¤í—˜ ê²°ê³¼

### 5.1 YOLOv8-Pose Training (Geometric Labels)

**ì„¤ì •**:
- Dataset: 50 train, 10 val
- Labels: Geometric PCA keypoints
- Training: 10 epochs, 4 batch size

**ê²°ê³¼**:
```
Box mAP50: 0.0012
Pose mAP50: 0.0000
Training time: 15 minutes
Model size: 7 MB
```

**ë¶„ì„**:
- âŒ **ì™„ì „ ì‹¤íŒ¨**: mAP ~0ì€ ëª¨ë¸ì´ í•™ìŠµ ëª»í•¨
- **ì›ì¸**: Label quality too low (geometric keypoints unreliable)
- **í•´ê²°ì±…**: Pretrained model (SuperAnimal) ë˜ëŠ” manual labeling í•„ìˆ˜

### 5.2 Geometric Detector (Baseline)

**From PoC (2025-11-14)**:
```
Processing time: ~1 second/image
Confidence: 0.40-0.70 (varies by keypoint)
Final loss: ~300K (very high)
Success rate: 100% (always returns keypoints)
```

**ì¥ì **:
- âœ… No training required
- âœ… Fast inference
- âœ… Always works (never fails)

**ë‹¨ì **:
- âŒ Low accuracy (especially paws)
- âŒ No anatomical knowledge
- âŒ High optimization loss

### 5.3 SuperAnimal (ì˜ˆìƒ ì„±ëŠ¥)

**Based on literature** (Ye et al. 2024, Nature Communications):
```
Dataset: 5K+ mice, diverse settings
Keypoints: 27 (comprehensive)
Accuracy: State-of-the-art for mice
mAP: Not reported, but proven in production
```

**ì˜ˆìƒ ê°œì„ **:
- Confidence: 0.40-0.70 â†’ **0.90+**
- Loss: 300K â†’ **15K-30K** (10-20Ã— improvement)
- Paw accuracy: Poor â†’ **Good** (anatomical knowledge)

---

## 6. ë¹„êµ ë¶„ì„

### 6.1 Method Comparison

| Method | Training | Accuracy | Speed | Robustness | Complexity |
|--------|----------|----------|-------|------------|------------|
| **Geometric** | None | Low (0.5) | Fast (1s) | High | Low |
| **YOLO (custom)** | Hours | **High*** | Very Fast (<0.1s) | Medium | Medium |
| **SuperAnimal** | Pretrained | **Highest** | Fast (0.5s) | High | High |

\* Requires quality labels (manual annotation í•„ìš”)

### 6.2 Trade-offs

**Geometric**:
- âœ… Pros: No setup, always works, fast
- âŒ Cons: Low accuracy, no learning

**YOLOv8-Pose**:
- âœ… Pros: Real-time inference, lightweight, flexible
- âŒ Cons: Requires quality training data (10-20 manual labels)

**SuperAnimal**:
- âœ… Pros: State-of-the-art, pretrained, proven
- âŒ Cons: DLC dependency, TensorFlow, keypoint mapping complexity

### 6.3 ê¶Œì¥ ì‚¬í•­

**Short-term (í˜„ì¬)**:
1. **SuperAnimal í†µí•©** (ì§„í–‰ ì¤‘) - Immediate improvement
2. Geometricì€ fallbackìœ¼ë¡œ ìœ ì§€

**Medium-term (1-2ì£¼)**:
3. Manual label 10-20 images
4. YOLO fine-tune
5. Compare SuperAnimal vs YOLO

**Long-term (1-2ê°œì›”)**:
6. Collect more data
7. Custom DLC training (if needed)

---

## 7. ë‹¤ìŒ ë‹¨ê³„

### 7.1 Immediate (ì´ë²ˆ ì„¸ì…˜)

âœ… **ì™„ë£Œ**:
1. YOLOv8-Pose infrastructure
2. DANNCE â†’ YOLO converter
3. SuperAnimal model download
4. Keypoint mapping analysis

ğŸš§ **ì§„í–‰ ì¤‘**:
5. SuperAnimal detector implementation
6. Test on sample images
7. Integration into fit_monocular.py

### 7.2 Short-term (1ì£¼)

**Phase 2 ì™„ë£Œ**:
1. SuperAnimalDetector class êµ¬í˜„
2. 27â†’22 keypoint mapping ê²€ì¦
3. fit_monocular.pyì— --detector flag ì¶”ê°€
4. Benchmark: geometric vs SuperAnimal

**ì˜ˆìƒ ê²°ê³¼**:
- Loss: 300K â†’ 20K-30K
- Confidence: 0.5 â†’ 0.90+
- Pose quality: T-pose bias â†’ realistic poses

### 7.3 Medium-term (2-4ì£¼)

**Manual Labeling + YOLO Fine-tuning**:
1. Label 10-20 representative images (CVAT)
2. Retrain YOLOv8-Pose
3. Compare SuperAnimal vs YOLO-finetune
4. Select best performer for production

**ì˜ˆìƒ ê²°ê³¼**:
- YOLO mAP: 0.000 â†’ 0.60-0.80
- Inference speed: SuperAnimal (0.5s) vs YOLO (<0.1s)

### 7.4 Long-term (Phase 3, optional)

**Custom DeepLabCut Training**:
- Train DLC on MAMMAL 22 keypoints (exact match)
- Use full DANNCE dataset (hundreds of images)
- Expected: Best possible accuracy

---

## 8. ê¸°ìˆ ì  êµí›ˆ

### 8.1 Dataset Quality is Everything

**í•µì‹¬ êµí›ˆ**: ML ëª¨ë¸ì€ ë°ì´í„° í’ˆì§ˆì— ì ˆëŒ€ì ìœ¼ë¡œ ì˜ì¡´
- Geometric keypointsë¡œ í•™ìŠµ â†’ mAP 0 (ì™„ì „ ì‹¤íŒ¨)
- Manual labels í•„ìš” (10-20 imagesë¡œë„ í° ì°¨ì´)
- Pretrained modelsê°€ gold standard

### 8.2 Transfer Learning > Training from Scratch

**ê´€ì°°**:
- YOLOv8-pose pretrained (COCO 17 keypoints) â†’ ìš°ë¦¬ 22 keypointsë¡œ fine-tune
- Architecture ìë™ ì¡°ì •: kpt_shape [17, 3] â†’ [22, 3]
- 361/397 weights transferred (91%)

**êµí›ˆ**: Always start with pretrained models

### 8.3 Keypoint Mapping Complexity

**Challenge**: SuperAnimal (27) â†’ MAMMAL (22)
- Direct mapping: 10/22 (45%)
- Interpolation: 9/22 (41%)
- Estimation: 3/22 (14%)

**Solution**: Implement robust interpolation + geometric inference

### 8.4 Environment Management

**Issue**: DeepLabCut (TensorFlow) vs Ultralytics (PyTorch)
- Different conda environments
- Dependency conflicts (numpy versions)

**Solution**: Separate environments or careful version management

---

## 9. ì½”ë“œ í•˜ì´ë¼ì´íŠ¸

### 9.1 DANNCE to YOLO Converter

**í•µì‹¬ ë¡œì§**: BBox clipping (critical!)

```python
def convert_bbox_to_yolo(self, bbox, img_width, img_height):
    x_min, y_min, x_max, y_max = bbox

    # Clip bbox to image bounds
    x_min = max(0, min(x_min, img_width - 1))
    y_min = max(0, min(y_min, img_height - 1))
    x_max = max(0, min(x_max, img_width - 1))
    y_max = max(0, min(y_max, img_height - 1))

    # Normalize to [0, 1]
    x_center_norm = (x_min + x_max) / 2.0 / img_width
    y_center_norm = (y_min + y_max) / 2.0 / img_height
    width_norm = (x_max - x_min) / img_width
    height_norm = (y_max - y_min) / img_height

    return [x_center_norm, y_center_norm, width_norm, height_norm]
```

**Without clipping**: 26/50 images rejected (negative coordinates)
**With clipping**: 50/50 images accepted âœ…

### 9.2 YOLOv8 Training Script

**í•µì‹¬ ì„¤ì •**:

```python
results = model.train(
    data='data.yaml',
    epochs=50,
    batch=8,
    imgsz=256,
    optimizer='Adam',
    lr0=0.001,
    # Augmentation (light for small dataset)
    fliplr=0.5,      # Horizontal flip with keypoint swapping
    degrees=10,      # Rotation
    scale=0.2,       # Scale jitter
    mosaic=0.5,      # Mosaic augmentation
)
```

**Key insight**: Light augmentation for small datasets

### 9.3 Keypoint Mapping (SuperAnimal â†’ MAMMAL)

**Interpolation helper** (ì˜ˆì •):

```python
def interpolate_keypoints(source_kpts, n_target):
    """
    Interpolate keypoints along backbone

    Args:
        source_kpts: List of (x, y, conf) tuples
        n_target: Target number of keypoints

    Returns:
        Interpolated keypoints (n_target, 3)
    """
    # Extract valid keypoints
    valid = [kpt for kpt in source_kpts if kpt[2] > 0.5]

    if len(valid) < 2:
        return np.zeros((n_target, 3))

    # Parameterize by cumulative distance
    positions = np.array([kpt[:2] for kpt in valid])
    distances = np.cumsum([0] + [np.linalg.norm(positions[i+1] - positions[i])
                                   for i in range(len(positions)-1)])

    # Interpolate
    t_interp = np.linspace(distances[0], distances[-1], n_target)
    x_interp = np.interp(t_interp, distances, positions[:, 0])
    y_interp = np.interp(t_interp, distances, positions[:, 1])
    conf_interp = np.full(n_target, np.mean([kpt[2] for kpt in valid]))

    return np.column_stack([x_interp, y_interp, conf_interp])
```

---

## 10. ê²°ë¡ 

### 10.1 í•µì‹¬ ì„±ê³¼

**Infrastructure ì™„ì„±**:
1. âœ… DANNCE â†’ YOLO ë³€í™˜ íŒŒì´í”„ë¼ì¸
2. âœ… YOLOv8-Pose í•™ìŠµ ì‹œìŠ¤í…œ
3. âœ… SuperAnimal ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„
4. âœ… Keypoint detector ì•„í‚¤í…ì²˜ ì„¤ê³„

**ì£¼ìš” ì¸ì‚¬ì´íŠ¸**:
- ML ëª¨ë¸ì€ ë°ì´í„° í’ˆì§ˆì— ì ˆëŒ€ì  ì˜ì¡´
- Pretrained models (SuperAnimal) í™œìš©ì´ critical
- Keypoint mappingì´ non-trivial but solvable

### 10.2 í˜„ì¬ ìƒíƒœ

**Baseline (Geometric)**:
- âœ… Working
- âš ï¸ Low accuracy (conf ~0.5, loss ~300K)

**YOLOv8-Pose**:
- âœ… Infrastructure complete
- âŒ Model quality low (needs quality labels)
- ğŸ“‹ TODO: Manual labeling (10-20 images)

**SuperAnimal**:
- âœ… Model downloaded (245 MB)
- âœ… Keypoint structure analyzed
- ğŸš§ Detector implementation in progress
- ğŸ“‹ Expected: 10-20Ã— improvement

### 10.3 ë‹¤ìŒ ì„¸ì…˜ ê³„íš

**Immediate (ì´ë²ˆ ì„¸ì…˜ ê³„ì†)**:
1. SuperAnimalDetector êµ¬í˜„
2. Test on sample images
3. Compare with geometric baseline

**Next Session**:
4. fit_monocular.py í†µí•©
5. Comprehensive benchmark
6. Final documentation

### 10.4 ì¥ê¸° ë¡œë“œë§µ

**Week 1-2**: SuperAnimal production integration
**Week 3-4**: Manual labeling + YOLO fine-tune
**Month 2-3**: Custom DLC training (optional)

---

## 11. ì°¸ê³  ìë£Œ

### 11.1 ë…¼ë¬¸

1. **SuperAnimal**: Ye et al., "SuperAnimal pretrained pose estimation models for behavioral analysis", Nature Communications (2024)
2. **MAMMAL**: An et al., "Three-dimensional surface motion capture of multiple freely moving pigs using MAMMAL" (2023)
3. **YOLOv8-Pose**: Ultralytics documentation

### 11.2 ì½”ë“œ ì €ì¥ì†Œ

**ì´ë²ˆ ì„¸ì…˜ ìƒì„±**:
- `preprocessing_utils/dannce_to_yolo.py` (329 lines)
- `preprocessing_utils/yolo_keypoint_detector.py` (368 lines)
- `train_yolo_pose.py` (121 lines)
- `download_superanimal.py` (35 lines)

**ë°ì´í„°ì…‹**:
- `data/yolo_mouse_pose/` (50 train + 10 val)

**ëª¨ë¸**:
- `runs/pose/mammal_mouse_test/weights/best.pt` (7 MB)
- `models/superanimal_topviewmouse/` (245 MB)

### 11.3 External Resources

- DeepLabCut Model Zoo: https://deeplabcut.github.io/DeepLabCut/docs/ModelZoo.html
- Ultralytics YOLO: https://docs.ultralytics.com/tasks/pose/
- HuggingFace: https://huggingface.co/mwmathis/DeepLabCutModelZoo-SuperAnimal-TopViewMouse

---

**ë³´ê³ ì„œ ì‘ì„±**: 2025-11-14
**Status**: Phase 1 ì™„ë£Œ, Phase 2 ì§„í–‰ ì¤‘
**Next**: SuperAnimal detector implementation
