# MAMMAL Mouse - Comprehensive Usage Guide

**μµμΆ… μ—…λ°μ΄νΈ**: 2025-11-15
**ν”„λ΅μ νΈ λ²„μ „**: v2.0 (ML Keypoint Detection + Monocular Fitting)

---

## π“– λ©μ°¨

1. [κ°μ”](#κ°μ”)
2. [ν”„λ΅μ νΈ κµ¬μ΅°](#ν”„λ΅μ νΈ-κµ¬μ΅°)
3. [ν™κ²½ μ„¤μ •](#ν™κ²½-μ„¤μ •)
4. [μ‚¬μ© μ‹λ‚λ¦¬μ¤λ³„ κ°€μ΄λ“](#μ‚¬μ©-μ‹λ‚λ¦¬μ¤λ³„-κ°€μ΄λ“)
5. [λ¨λ“  κΈ°λ¥ μƒμ„Έ μ„¤λ…](#λ¨λ“ -κΈ°λ¥-μƒμ„Έ-μ„¤λ…)
6. [κ³ κΈ‰ μ‚¬μ©λ²•](#κ³ κΈ‰-μ‚¬μ©λ²•)
7. [λ¬Έμ  ν•΄κ²°](#λ¬Έμ -ν•΄κ²°)
8. [μ°Έκ³  μλ£](#μ°Έκ³ -μλ£)

---

## κ°μ”

MAMMAL Mouseλ” λ§μ°μ¤μ 3D μμ„Έ μ¶”μ • λ° λ©”μ‰¬ μ¬κµ¬μ„±μ„ μ„ν• ν†µν•© ν”„λ μ„μ›ν¬μ…λ‹λ‹¤.

### μ£Όμ” κΈ°λ¥

| κΈ°λ¥ | μ„¤λ… | μƒνƒ |
|------|------|------|
| **Multi-view Fitting** | λ‹¤μ¤‘ μΉ΄λ©”λΌ λ™κΈ°ν™” μμƒμ—μ„ 3D ν”Όν… | β… μ•μ • |
| **Monocular Fitting** | λ‹¨μΌ μΉ΄λ©”λΌ μμƒμ—μ„ μ§μ ‘ 3D ν”Όν… | π†• μ‹ κ· |
| **ML Keypoint Detection** | YOLOv8, SuperAnimal κΈ°λ° ν‚¤ν¬μΈνΈ κ²€μ¶ | π†• μ‹ κ· |
| **Geometric Baseline** | PCA κΈ°λ° κΈ°ν•ν•™μ  ν‚¤ν¬μΈνΈ μ¶”μ • | β… μ•μ • |
| **Manual Labeling Workflow** | Roboflow κΈ°λ° μλ™ λΌλ²¨λ§ νμ΄ν”„λΌμΈ | π†• μ‹ κ· |
| **Hydra Configuration** | μ μ—°ν• μ‹¤ν— κ΄€λ¦¬ μ‹μ¤ν… | β… μ•μ • |

### μ§€μ›ν•λ” μ…λ ¥ ν•μ‹

- **Multi-view**: λ™κΈ°ν™”λ λ‹¤μ¤‘ μΉ΄λ©”λΌ μμƒ + 2D ν‚¤ν¬μΈνΈ + μ‹¤λ£¨μ—£ λ§μ¤ν¬
- **Monocular**: λ‹¨μΌ μΉ΄λ©”λΌ μμƒ ν”„λ μ„ (PNG, JPG)
- **Preprocessing**: μ›λ³Έ λΉ„λ””μ¤ (MP4, AVI λ“±)

---

## ν”„λ΅μ νΈ κµ¬μ΅°

### μµμΆ… μ •λ¦¬λ κµ¬μ΅° (2025-11-15)

```
MAMMAL_mouse/
β”β”€β”€ README.md                    # ν”„λ΅μ νΈ κ°μ”
β”β”€β”€ requirements.txt             # Python μμ΅΄μ„±
β”‚
β”β”€β”€ π“ conf/                     # Hydra μ„¤μ • νμΌ
β”‚   β”β”€β”€ config.yaml              # λ©”μΈ μ„¤μ •
β”‚   β”β”€β”€ dataset/                 # λ°μ΄ν„°μ…‹λ³„ μ„¤μ •
β”‚   β”β”€β”€ preprocess/              # μ „μ²λ¦¬ λ°©λ²• μ„¤μ •
β”‚   β””β”€β”€ optim/                   # μµμ ν™” μ„¤μ •
β”‚
β”β”€β”€ π Python Scripts (λ£¨νΈ)    # μ‹¤ν–‰ κ°€λ¥ν• λ©”μΈ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ fitter_articulation.py   # λ©”μΈ ν”Όν… μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ fit_monocular.py         # π†• λ¨λ…Ένλ¬ ν”Όν…
β”‚   β”β”€β”€ preprocess.py            # μ „μ²λ¦¬ νμ΄ν”„λΌμΈ
β”‚   β”β”€β”€ train_yolo_pose.py       # π†• YOLO ν•™μµ
β”‚   β”β”€β”€ articulation_th.py       # κ΄€μ  λ¨λΈ
β”‚   β”β”€β”€ bodymodel_th.py          # λ°”λ”” λ¨λΈ
β”‚   β””β”€β”€ utils.py                 # μ ν‹Έλ¦¬ν‹° ν•¨μ
β”‚
β”β”€β”€ π“¦ preprocessing_utils/      # μ „μ²λ¦¬ λ¨λ“
β”‚   β”β”€β”€ keypoint_estimation.py   # κΈ°ν•ν•™μ  ν‚¤ν¬μΈνΈ μ¶”μ •
β”‚   β”β”€β”€ mask_processing.py       # λ§μ¤ν¬ μ²λ¦¬
β”‚   β”β”€β”€ yolo_keypoint_detector.py    # π†• YOLO κ²€μ¶κΈ°
β”‚   β”β”€β”€ superanimal_detector.py      # π†• SuperAnimal κ²€μ¶κΈ°
β”‚   β”β”€β”€ dannce_to_yolo.py            # π†• λ°μ΄ν„°μ…‹ λ³€ν™
β”‚   β””β”€β”€ visualize_yolo_labels.py     # π†• λΌλ²¨ μ‹κ°ν™”
β”‚
β”β”€β”€ π’Ύ data/                     # λ°μ΄ν„°μ…‹
β”‚   β”β”€β”€ raw/                     # μ›λ³Έ λ°μ΄ν„°
β”‚   β”β”€β”€ preprocessed/            # μ „μ²λ¦¬ κ²°κ³Ό
β”‚   β”β”€β”€ training/                # ML ν•™μµ λ°μ΄ν„°
β”‚   β”‚   β”β”€β”€ yolo_mouse_pose/     # YOLO λ°μ΄ν„°μ…‹
β”‚   β”‚   β””β”€β”€ manual_labeling/     # μλ™ λΌλ²¨λ§ (μ§„ν–‰ μ¤‘)
β”‚   β””β”€β”€ examples/                # μμ  λ°μ΄ν„°
β”‚
β”β”€β”€ π¤– models/                   # λ¨λΈ κ°€μ¤‘μΉ
β”‚   β”β”€β”€ pretrained/              # μ‚¬μ „ν•™μµ λ¨λΈ
β”‚   β”‚   β”β”€β”€ superanimal_topviewmouse/  # SuperAnimal
β”‚   β”‚   β”β”€β”€ sam/                 # SAM (Segment Anything)
β”‚   β”‚   β”β”€β”€ yolov8n-pose.pt      # YOLOv8 κΈ°λ³Έ
β”‚   β”‚   β””β”€β”€ yolo11n.pt           # YOLO11 κΈ°λ³Έ
β”‚   β””β”€β”€ trained/                 # ν•™μµλ λ¨λΈ
β”‚       β””β”€β”€ yolo/                # YOLO ν•™μµ κ²°κ³Ό
β”‚
β”β”€β”€ π“ results/                  # μµμ‹  μ‹¤ν— κ²°κ³Ό
β”‚   β”β”€β”€ monocular/               # λ¨λ…Ένλ¬ ν”Όν… κ²°κ³Ό
β”‚   β””β”€β”€ preprocessing/           # μ „μ²λ¦¬ κ²°κ³Ό
β”‚
β”β”€β”€ π“ outputs/                  # Hydra μλ™ μƒμ„±
β”‚   β””β”€β”€ archives/                # μ¤λλ μ‹¤ν— μ•„μΉ΄μ΄λΈ
β”‚
β”β”€β”€ π“ docs/                     # λ¬Έμ„
β”‚   β”β”€β”€ guides/                  # μ‚¬μ© κ°€μ΄λ“ (7κ°)
β”‚   β”‚   β”β”€β”€ MONOCULAR_FITTING_GUIDE.md
β”‚   β”‚   β”β”€β”€ QUICK_START_LABELING.md
β”‚   β”‚   β”β”€β”€ ROBOFLOW_LABELING_GUIDE.md
β”‚   β”‚   β”β”€β”€ SAM_MASK_ACQUISITION_MANUAL.md
β”‚   β”‚   β””β”€β”€ MAMMAL_ARCHITECTURE_MANUAL.md
β”‚   β””β”€β”€ reports/                 # μ—°κµ¬ λ³΄κ³ μ„ (11κ°)
β”‚       β”β”€β”€ 251114_ml_keypoint_detection_integration.md
β”‚       β”β”€β”€ 251115_comprehensive_ml_keypoint_summary.md
β”‚       β””β”€β”€ ... (κΈ°νƒ€ μ„Έμ… λ³΄κ³ μ„)
β”‚
β”β”€β”€ π¨ assets/                   # μ •μ  λ¦¬μ†μ¤
β”‚   β”β”€β”€ colormaps/               # μ‹κ°ν™” μ»¬λ¬λ§µ
β”‚   β”β”€β”€ figs/                    # README μ΄λ―Έμ§€
β”‚   β””β”€β”€ mouse_model/             # 3D λ§μ°μ¤ λ¨λΈ
β”‚       β”β”€β”€ mouse_reduced_face_*.obj
β”‚       β””β”€β”€ mouse_txt/           # λ¨λΈ νλΌλ―Έν„°
β”‚
β””β”€β”€ π§ tests/                    # ν…μ¤νΈ μ¤ν¬λ¦½νΈ
    β”β”€β”€ test_sam.py
    β”β”€β”€ test_superanimal.py
    β””β”€β”€ ... (κΈ°νƒ€ ν…μ¤νΈ)
```

---

## ν™κ²½ μ„¤μ •

### 1ν μ„¤μ • (μ²μ μ‚¬μ© μ‹)

```bash
# 1. λ¦¬ν¬μ§€ν† λ¦¬ ν΄λ΅ 
git clone <repository_url>
cd MAMMAL_mouse

# 2. ν™κ²½ μ„¤μ • μ¤ν¬λ¦½νΈ μ‹¤ν–‰
bash setup.sh
```

### ν™κ²½ ν™μ„±ν™”

```bash
conda activate mammal_stable
```

### μμ΅΄μ„± ν™•μΈ

```bash
# Python ν™κ²½ ν™•μΈ
python --version  # Python 3.10

# PyTorch λ° CUDA ν™•μΈ
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"

# μ£Όμ” ν¨ν‚¤μ§€ ν™•μΈ
python -c "import pytorch3d, hydra, ultralytics; print('All packages OK')"
```

---

## μ‚¬μ© μ‹λ‚λ¦¬μ¤λ³„ κ°€μ΄λ“

### μ‹λ‚λ¦¬μ¤ 1: λ‹¨μΌ μμƒμ—μ„ λΉ λ¥Έ 3D ν”Όν… (NEW! β­ μ¶”μ²)

**λ©μ **: λ‹¨μΌ μΉ΄λ©”λΌ μμƒμ—μ„ λΉ λ¥΄κ² 3D μμ„Έ μ¶”μ •

**μ…λ ¥**:
- μμƒ ν”„λ μ„ (PNG, JPG)
- λλ” μ „μ²λ¦¬λ λ§μ¤ν¬ + ν‚¤ν¬μΈνΈ

**μμƒ μ‹κ°„**: ~30μ΄/ν”„λ μ„

**μ‹¤ν–‰ λ°©λ²•**:

```bash
# Step 1: μμƒ ν”„λ μ„ μ¤€λΉ„
# - ν”„λ μ„ μ¶”μ¶μ΄ ν•„μ”ν• κ²½μ°:
ffmpeg -i video.mp4 -vf fps=10 frames/frame_%04d.png

# Step 2: Monocular fitting μ‹¤ν–‰
python fit_monocular.py \
  --input_dir frames/ \
  --output_dir results/monocular/my_experiment \
  --detector geometric \
  --max_images 10

# μµμ…:
# --detector geometric|yolo|superanimal
# --max_images N (μ²λ¦¬ν•  ν”„λ μ„ μ μ ν•)
# --yolo_weights path/to/weights.pt (YOLO μ‚¬μ© μ‹)
```

**κ²°κ³Ό ν™•μΈ**:
```bash
ls results/monocular/my_experiment/
# - frame_0000_mesh.obj: 3D λ©”μ‰¬
# - frame_0000_keypoints.png: ν‚¤ν¬μΈνΈ μ‹κ°ν™”
# - ... (κ° ν”„λ μ„λ³„ κ²°κ³Ό)
```

**μƒμ„Έ κ°€μ΄λ“**: `docs/guides/MONOCULAR_FITTING_GUIDE.md`

---

### μ‹λ‚λ¦¬μ¤ 2: ML κΈ°λ° κ³ ν’μ§ ν‚¤ν¬μΈνΈ κ²€μ¶ (NEW! π“)

**λ©μ **: YOLOv8μ„ μλ™ λΌλ²¨λ§ν•μ—¬ κ³ ν’μ§ ν‚¤ν¬μΈνΈ κ²€μ¶κΈ° κµ¬μ¶•

**μμƒ κ°μ„ **: Confidence 2Γ—, Loss 10-20Γ—, mAP 0β†’0.6-0.8

**μ „μ²΄ μ›ν¬ν”λ΅μ°** (~3-4μ‹κ°„):

#### Step 1: μ΄λ―Έμ§€ μƒν”λ§ (μ™„λ£λ¨)
```bash
# μ΄λ―Έ 20κ° μ΄λ―Έμ§€ μ¤€λΉ„λ¨
ls data/training/manual_labeling/images/
# sample_000.png ~ sample_019.png
```

#### Step 2: Roboflowμ—μ„ λΌλ²¨λ§ (2-3μ‹κ°„)
1. https://roboflow.com/ μ ‘μ† λ° κ°€μ…
2. ν”„λ΅μ νΈ μƒμ„±: "MAMMAL_Mouse_Keypoints" (Keypoint Detection)
3. 22κ° keypoints μ •μ (μ •ν™•ν• μμ„!):
   ```
   0: nose, 1: left_ear, 2: right_ear, 3: left_eye, 4: right_eye,
   5: head_center, 6-13: spine_1 to spine_8,
   14-17: paws (left/right, front/rear),
   18-20: tail (base/mid/tip), 21: centroid
   ```
4. 20κ° μ΄λ―Έμ§€ μ—…λ΅λ“ λ° λΌλ²¨λ§
5. YOLO v8 formatμΌλ΅ export

#### Step 3: λΌλ²¨ κ²€μ¦ (5λ¶„)
```bash
# Roboflow export μ••μ¶• ν•΄μ 
cd ~/Downloads
unzip roboflow.zip -d ~/dev/MAMMAL_mouse/data/training/manual_labeling/roboflow_export

# λΌλ²¨ λ³µμ‚¬
cp -r data/training/manual_labeling/roboflow_export/train/labels/* \
      data/training/manual_labeling/labels/

# μ‹κ°ν™” κ²€μ¦
python preprocessing_utils/visualize_yolo_labels.py \
  --images data/training/manual_labeling/images \
  --labels data/training/manual_labeling/labels \
  --output data/training/manual_labeling/viz \
  --max_images 5

# κ²°κ³Ό ν™•μΈ
ls data/training/manual_labeling/viz/
```

#### Step 4: λ°μ΄ν„°μ…‹ λ³‘ν•© (2λ¶„)
```bash
# Manual (20) + Geometric (50) = 70 images
python preprocessing_utils/merge_datasets.py \
  --manual data/training/manual_labeling \
  --geometric data/training/yolo_mouse_pose \
  --output data/training/yolo_mouse_pose_enhanced \
  --train_split 0.8
```

#### Step 5: YOLOv8 ν•™μµ (30λ¶„)
```bash
python scripts/train_yolo_pose.py \
  --data data/training/yolo_mouse_pose_enhanced/data.yaml \
  --epochs 100 \
  --batch 8 \
  --imgsz 256 \
  --weights models/pretrained/yolov8n-pose.pt \
  --name mammal_mouse_finetuned

# ν•™μµ λ¨λ‹ν„°λ§ (λ‹¤λ¥Έ ν„°λ―Έλ„)
tail -f /tmp/yolo_train.log
```

#### Step 6: ν‰κ°€ (10λ¶„)
```bash
# Validation ν‰κ°€
python -c "
from ultralytics import YOLO
model = YOLO('models/trained/yolo/mammal_mouse_finetuned/weights/best.pt')
metrics = model.val(data='data/training/yolo_mouse_pose_enhanced/data.yaml')
print(f'mAP50: {metrics.box.map50:.3f}')
print(f'mAP50-95: {metrics.box.map:.3f}')
"

# μ‹κ°μ  λΉ„κµ
python fit_monocular.py \
  --input_dir data/training/manual_labeling/images \
  --output_dir results/yolo_comparison \
  --detector yolo \
  --yolo_weights models/trained/yolo/mammal_mouse_finetuned/weights/best.pt \
  --max_images 5
```

#### Step 7: Production ν†µν•© (5λ¶„)
```bash
# Best model λ³µμ‚¬
mkdir -p models/production
cp models/trained/yolo/mammal_mouse_finetuned/weights/best.pt \
   models/production/yolo_mouse_pose_finetuned.pt

# μ΄ν›„ μ‚¬μ©
python fit_monocular.py \
  --detector yolo \
  --yolo_weights models/production/yolo_mouse_pose_finetuned.pt
```

**μƒμ„Έ κ°€μ΄λ“**:
- `docs/guides/QUICK_START_LABELING.md`
- `docs/guides/ROBOFLOW_LABELING_GUIDE.md`
- `docs/reports/251115_comprehensive_ml_keypoint_summary.md`

---

### μ‹λ‚λ¦¬μ¤ 3: λ‹¤μ¤‘ μΉ΄λ©”λΌ μμƒμ—μ„ 3D ν”Όν… (κΈ°λ³Έ)

**λ©μ **: λ™κΈ°ν™”λ λ‹¤μ¤‘ μΉ΄λ©”λΌ μμƒμ—μ„ μ •λ°€ν• 3D μμ„Έ μ¶”μ •

**μ…λ ¥**:
- λ‹¤μ¤‘ μΉ΄λ©”λΌ λ™κΈ°ν™” μμƒ
- 2D ν‚¤ν¬μΈνΈ (κ° view)
- μ‹¤λ£¨μ—£ λ§μ¤ν¬ (κ° view)
- μΉ΄λ©”λΌ νλΌλ―Έν„°

**μ‹¤ν–‰ λ°©λ²•**:

```bash
# μμ  λ°μ΄ν„°μ…‹ μ‚¬μ© (markerless_mouse_1)
python fitter_articulation.py \
  dataset=markerless \
  optim=fast \
  fitter.end_frame=10

# μ»¤μ¤ν…€ λ°μ΄ν„°μ…‹
python fitter_articulation.py \
  dataset=custom \
  data.data_dir=data/preprocessed/my_dataset/ \
  fitter.end_frame=100
```

**κ²°κ³Ό ν™•μΈ**:
```bash
ls outputs/YYYY-MM-DD/HH-MM-SS/
# Hydraκ°€ μλ™μΌλ΅ νƒ€μ„μ¤νƒ¬ν”„ ν΄λ” μƒμ„±
```

---

### μ‹λ‚λ¦¬μ¤ 4: μ›λ³Έ λΉ„λ””μ¤ μ „μ²λ¦¬ (λ‹¨μΌ μΉ΄λ©”λΌ)

**λ©μ **: μ›λ³Έ λΉ„λ””μ¤λ¥Ό MAMMAL μ…λ ¥ ν•μ‹μΌλ΅ λ³€ν™

**μ…λ ¥**: λΉ„λ””μ¤ νμΌ (MP4, AVI λ“±)

**μ¶λ ¥**:
- `videos_undist/0.mp4`: μ›λ³Έ λΉ„λ””μ¤
- `simpleclick_undist/0.mp4`: λ§μ¤ν¬ λΉ„λ””μ¤
- `keypoints2d_undist/result_view_0.pkl`: 2D ν‚¤ν¬μΈνΈ
- `new_cam.pkl`: μΉ΄λ©”λΌ νλΌλ―Έν„°

**μ‹¤ν–‰ λ°©λ²•**:

```bash
# Step 1: μ„¤μ • νμΌ μ¤€λΉ„
# conf/dataset/my_video.yaml μƒμ„±:
cat > conf/dataset/my_video.yaml << 'EOF'
# @package _global_

data:
  data_dir: data/preprocessed/my_video/
  views_to_use: [0]

preprocess:
  input_video_path: data/raw/my_video.mp4
  output_data_dir: data/preprocessed/my_video/

fitter:
  start_frame: 0
  end_frame: 100
  render_cameras: [0]
EOF

# Step 2: μ „μ²λ¦¬ μ‹¤ν–‰
python scripts/preprocess.py \
  dataset=my_video \
  mode=single_view_preprocess

# Step 3: κ²°κ³Ό ν™•μΈ
ls data/preprocessed/my_video/
```

**μ „μ²λ¦¬ ν›„ ν”Όν…**:
```bash
python fitter_articulation.py \
  dataset=my_video \
  mode=multi_view
```

---

### μ‹λ‚λ¦¬μ¤ 5: κΈ°ν•ν•™μ  λ² μ΄μ¤λΌμΈ μ‚¬μ©

**λ©μ **: λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘ λ° λ² μ΄μ¤λΌμΈ λΉ„κµ

**μ¥μ **:
- λ¨λΈ ν•™μµ λ¶ν•„μ”
- λΉ λ¥Έ μ‹¤ν–‰
- μ „μ²λ¦¬ λ‹¨κ³„ μ—†μ

**λ‹¨μ **:
- μ •ν™•λ„ λ‚®μ (νΉν paw detection)
- Confidence λ‚®μ (0.4-0.6)

**μ‚¬μ© λ°©λ²•**:

```bash
# Monocular fitting with geometric detector
python fit_monocular.py \
  --input_dir frames/ \
  --output_dir results/geometric_baseline \
  --detector geometric

# λλ” μ „μ²λ¦¬ λ‹¨κ³„μ—μ„
python scripts/preprocess.py \
  dataset=my_video \
  preprocess.method=opencv  # κΈ°ν•ν•™μ  λ°©λ²•
```

**μμƒ μ„±λ¥**:
- Detected keypoints: 15/22
- Confidence: 0.4-0.6
- Loss: ~300K

---

## λ¨λ“  κΈ°λ¥ μƒμ„Έ μ„¤λ…

### 1. Keypoint Detector μµμ…

#### Geometric (κΈ°ν•ν•™μ )
- **νμΌ**: `preprocessing_utils/keypoint_estimation.py`
- **λ°©λ²•**: PCA κΈ°λ° contour λ¶„μ„
- **μ¥μ **: λΉ λ¦„, ν•™μµ λ¶ν•„μ”
- **λ‹¨μ **: μ •ν™•λ„ λ‚®μ
- **μ‚¬μ© μ‹κΈ°**: λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘

#### YOLOv8-Pose
- **νμΌ**: `preprocessing_utils/yolo_keypoint_detector.py`
- **λ°©λ²•**: CNN κΈ°λ° ν‚¤ν¬μΈνΈ κ²€μ¶
- **μ¥μ **: λΉ λ¥΄κ³  μ •ν™•, GPU κ°€μ†
- **λ‹¨μ **: ν•™μµ λ°μ΄ν„° ν•„μ”
- **μ‚¬μ© μ‹κΈ°**: Production, μ‹¤μ‹κ°„ μ²λ¦¬

**μ‚¬μ©λ²•**:
```bash
python fit_monocular.py \
  --detector yolo \
  --yolo_weights models/production/yolo_mouse_pose_finetuned.pt
```

#### SuperAnimal-TopViewMouse
- **νμΌ**: `preprocessing_utils/superanimal_detector.py`
- **λ°©λ²•**: DeepLabCut pretrained model (27 keypoints β†’ 22 mapping)
- **μ¥μ **: μ‚¬μ „ν•™μµ, ν•΄λ¶€ν•™μ  μ •ν™•λ„
- **λ‹¨μ **: DLC API μ μ•½, λλ¦Ό
- **μƒνƒ**: Geometric fallback μ‚¬μ© μ¤‘ (DLC 3.0 λ€κΈ°)

**μ‚¬μ©λ²•**:
```bash
python fit_monocular.py \
  --detector superanimal \
  --superanimal_model models/pretrained/superanimal_topviewmouse
```

### 2. Hydra Configuration μ‹μ¤ν…

#### μ„¤μ • νμΌ κµ¬μ΅°
```
conf/
β”β”€β”€ config.yaml          # λ©”μΈ μ„¤μ • (defaults)
β”β”€β”€ dataset/             # λ°μ΄ν„°μ…‹λ³„ μ„¤μ •
β”‚   β”β”€β”€ markerless.yaml  # μ: 6-view multi-camera
β”‚   β”β”€β”€ shank3.yaml      # μ: single-view
β”‚   β””β”€β”€ custom.yaml      # ν…ν”λ¦Ώ
β”β”€β”€ preprocess/          # μ „μ²λ¦¬ λ°©λ²•
β”‚   β”β”€β”€ opencv.yaml      # Geometric baseline
β”‚   β””β”€β”€ sam.yaml         # SAM (ν–¥ν›„)
β””β”€β”€ optim/               # μµμ ν™” μ„¤μ •
    β”β”€β”€ fast.yaml        # λΉ λ¥Έ ν…μ¤νΈ (μ μ€ iteration)
    β””β”€β”€ accurate.yaml    # μ •λ°€ κ²°κ³Ό (λ§μ€ iteration)
```

#### μ„¤μ • μ΅°ν•© μμ‹

**λΉ λ¥Έ ν…μ¤νΈ**:
```bash
python fitter_articulation.py \
  dataset=markerless \
  optim=fast \
  fitter.end_frame=5
```

**μ •λ°€ ν”Όν…**:
```bash
python fitter_articulation.py \
  dataset=custom \
  optim=accurate \
  fitter.with_render=true
```

**νλΌλ―Έν„° μ¤λ²„λΌμ΄λ“**:
```bash
python fitter_articulation.py \
  dataset=shank3 \
  fitter.start_frame=10 \
  fitter.end_frame=50 \
  optim.solve_step1_iters=200
```

### 3. 3D Fitting Pipeline

#### 3λ‹¨κ³„ μµμ ν™”

**Step 0: μ΄κΈ° μμ„Έ μ¶”μ •** (10 iterations)
- Global translation/rotation μ΄κΈ°ν™”
- λ€λµμ μΈ μμ„Έ λ§μ¶¤

**Step 1: ν‚¤ν¬μΈνΈ κΈ°λ° ν”Όν…** (100 iterations)
- 2D ν‚¤ν¬μΈνΈμ™€ 3D λ¨λΈ μ •λ ¬
- κ΄€μ  κ°λ„ μµμ ν™”
- Loss terms: 2D reprojection, 3D keypoint, bone length

**Step 2: Silhouette κΈ°λ° μ •λ°€ν™”** (30 iterations)
- PyTorch3Dλ¥Ό μ‚¬μ©ν• μ‹¤λ£¨μ—£ λ§¤μΉ­
- λ©”μ‰¬ ν‘λ©΄ μµμ ν™”
- Loss terms: silhouette IoU, smoothness

#### Loss Terms κ°€μ¤‘μΉ

`fitter_articulation.py` λΌμΈ ~82:
```python
self.term_weights = {
    "theta": 3,           # κ΄€μ  μ •κ·ν™”
    "3d": 2.5,            # 3D ν‚¤ν¬μΈνΈ loss
    "2d": 0.2,            # 2D μ¬ν¬μ loss
    "bone": 0.5,          # λΌ κΈΈμ΄ μ μ•½
    "scale": 0.5,         # μ¤μΌ€μΌ μ •κ·ν™”
    "mask": 0,            # μ‹¤λ£¨μ—£ loss (κΈ°λ³Έ λΉ„ν™μ„±ν™”)
    "chest_deformer": 0.1,  # κ°€μ΄ λ³€ν• μ •κ·ν™”
    "stretch": 1,         # λμ–΄λ‚¨ νλ„ν‹°
    "temp": 0.25,         # μ‹κ°„μ  λ¶€λ“λ¬μ›€
    "temp_d": 0.2         # μ‹κ°„ λ―Έλ¶„ λ¶€λ“λ¬μ›€
}
```

### 4. μ‹κ°ν™” λ° μ¶λ ¥

#### μ¶λ ¥ νμΌ κµ¬μ΅°

**Monocular Fitting**:
```
results/monocular/my_experiment/
β”β”€β”€ frame_0000_mesh.obj           # 3D λ©”μ‰¬
β”β”€β”€ frame_0000_keypoints.png      # ν‚¤ν¬μΈνΈ μ¤λ²„λ μ΄
β”β”€β”€ frame_0000_params.pkl         # ν”Όν… νλΌλ―Έν„°
β””β”€β”€ ...
```

**Multi-view Fitting**:
```
outputs/YYYY-MM-DD/HH-MM-SS/
β””β”€β”€ (Hydra μλ™ μƒμ„±)

κ²°κ³Όλ” μ‹¤μ λ΅ μ €μ¥λ¨:
results/obj/                      # 3D λ©”μ‰¬
results/params/                   # ν”Όν… νλΌλ―Έν„°
results/render/                   # μ‹κ°ν™”
```

#### λΉ„λ””μ¤ μƒμ„±

```bash
# λ λ”λ§ μ΄λ―Έμ§€μ—μ„ λΉ„λ””μ¤ μƒμ„±
ffmpeg -framerate 10 \
  -i results/monocular/my_experiment/frame_%04d_keypoints.png \
  -c:v libx264 -pix_fmt yuv420p -y output.mp4
```

---

## κ³ κΈ‰ μ‚¬μ©λ²•

### 1. μ»¤μ¤ν…€ ν‚¤ν¬μΈνΈ κ°€μ¤‘μΉ μ΅°μ •

`fitter_articulation.py` λΌμΈ ~65:
```python
self.keypoint_weight = np.ones(22)

# μ‹ λΆ°λ„ λ‚®μ€ ν‚¤ν¬μΈνΈ κ°€μ¤‘μΉ κ°μ†
self.keypoint_weight[4] = 0.4   # right_ear
self.keypoint_weight[11] = 0.9  # left_hip
self.keypoint_weight[15] = 0.9  # left_foot

# λλ” νΉμ • ν‚¤ν¬μΈνΈ λ¬΄μ‹
self.keypoint_weight[14:18] = 0  # paws (geometricμ΄ μ λ»μ΅μ„ λ•)
```

### 2. SAMμ„ μ΄μ©ν• κ³ ν’μ§ λ§μ¤ν¬ μƒμ„±

```bash
# SAMμΌλ΅ λ§μ¤ν¬ μƒμ„± (ν–¥ν›„ κΈ°λ¥)
python tests/sam_point_prompt.py \
  --image frame_0001.png \
  --output mask_0001.png

# λλ” batch processing
python preprocessing_utils/sam_inference.py \
  --input_dir frames/ \
  --output_dir masks/
```

**μƒμ„Έ κ°€μ΄λ“**: `docs/guides/SAM_MASK_ACQUISITION_MANUAL.md`

### 3. λ°°μΉ μ²λ¦¬

```bash
# μ—¬λ¬ λ°μ΄ν„°μ…‹ μμ°¨ μ²λ¦¬
for dataset in mouse1 mouse2 mouse3; do
  python fit_monocular.py \
    --input_dir data/${dataset}/frames \
    --output_dir results/monocular/${dataset} \
    --detector yolo \
    --max_images 100
done
```

### 4. GPU λ©”λ¨λ¦¬ μµμ ν™”

```bash
# Batch size κ°μ†
python scripts/train_yolo_pose.py --batch 4  # default: 8

# μ΄λ―Έμ§€ ν¬κΈ° κ°μ†
python scripts/train_yolo_pose.py --imgsz 192  # default: 256

# Fittingμ‹ λ λ”λ§ λΉ„ν™μ„±ν™”
python fitter_articulation.py fitter.with_render=false
```

---

## λ¬Έμ  ν•΄κ²°

### ν™κ²½ κ΄€λ ¨

**Q: `ModuleNotFoundError: No module named 'torch'`**
```bash
# ν•΄κ²°: ν™κ²½ μ¬μ„¤μΉ
bash setup.sh
conda activate mammal_stable
```

**Q: `CUDA out of memory`**
```bash
# ν•΄κ²° 1: Batch size κ°μ†
python scripts/train_yolo_pose.py --batch 2

# ν•΄κ²° 2: ν”„λ μ„ μ μ ν•
python fit_monocular.py --max_images 10

# ν•΄κ²° 3: GPU λ©”λ¨λ¦¬ ν™•μΈ
nvidia-smi
```

### Keypoint Detection κ΄€λ ¨

**Q: Geometric detectorκ°€ pawsλ¥Ό λ» μ°Ύμ**
- **λ‹µλ³€**: μ •μƒμ…λ‹λ‹¤. Geometric λ°©λ²•μ€ PCA κΈ°λ°μ΄λΌ μ‚¬μ§€(paws)κ²€μ¶ λ¶κ°€λ¥
- **ν•΄κ²°**: YOLO fine-tuning λλ” SuperAnimal μ‚¬μ©

**Q: YOLO ν•™μµ κ²°κ³Όκ°€ mAP ~0**
- **λ‹µλ³€**: Geometric labelsλ΅ ν•™μµν•λ©΄ λ°μƒ
- **ν•΄κ²°**: Manual labeling (20κ°) μν–‰ ν›„ μ¬ν•™μµ

**Q: SuperAnimalμ΄ μ‘λ™ν•μ§€ μ•μ**
- **λ‹µλ³€**: DLC 2.3.11 TensorFlow API μ μ•½
- **ν•΄κ²°**: Geometric fallback μ‚¬μ© μ¤‘, DLC 3.0 λ¦΄λ¦¬μ¤ λ€κΈ°

### Fitting κ΄€λ ¨

**Q: Fittingμ΄ μ΄μƒν• μμ„Έλ΅ μλ ΄**
```bash
# ν•΄κ²° 1: λ” λ§μ€ iteration
python fitter_articulation.py optim=accurate

# ν•΄κ²° 2: μ‹μ‘ ν”„λ μ„ λ³€κ²½
python fitter_articulation.py fitter.start_frame=10

# ν•΄κ²° 3: ν‚¤ν¬μΈνΈ ν’μ§ ν™•μΈ
python preprocessing_utils/visualize_yolo_labels.py ...
```

**Q: `FileNotFoundError: new_cam.pkl not found`**
```bash
# ν•΄κ²°: μ „μ²λ¦¬ λ¨Όμ € μ‹¤ν–‰
python scripts/preprocess.py dataset=my_video mode=single_view_preprocess
```

### Rendering κ΄€λ ¨

**Q: `NoSuchDisplayException: Cannot connect to "None"`**
- **λ‹µλ³€**: μ΄λ―Έ μ²λ¦¬λ¨ (`export PYOPENGL_PLATFORM=egl`)
- **ν™•μΈ**: `ldconfig -p | grep EGL`

**Q: λ λ”λ§ μ΄λ―Έμ§€κ°€ κ²€μ€μƒ‰**
```bash
# ν•΄κ²°: λ λ”λ§ λΉ„ν™μ„±ν™”ν•κ³  λ””λ²„κΉ…
python fitter_articulation.py fitter.with_render=false
```

---

## μ°Έκ³  μλ£

### λ¬Έμ„ (docs/guides/)
1. **MONOCULAR_FITTING_GUIDE.md** - λ¨λ…Ένλ¬ ν”Όν… μƒμ„Έ κ°€μ΄λ“
2. **QUICK_START_LABELING.md** - μλ™ λΌλ²¨λ§ λΉ λ¥Έ μ‹μ‘
3. **ROBOFLOW_LABELING_GUIDE.md** - Roboflow λ‹¨κ³„λ³„ κ°€μ΄λ“
4. **SAM_MASK_ACQUISITION_MANUAL.md** - SAM λ§μ¤ν¬ νλ“ λ°©λ²•
5. **MAMMAL_ARCHITECTURE_MANUAL.md** - μ „μ²΄ μ•„ν‚¤ν…μ² μƒμ„Έ μ„¤λ…

### μ—°κµ¬ λ³΄κ³ μ„ (docs/reports/)
1. **251114_ml_keypoint_detection_integration.md** - ML ν†µν•© κΈ°μ  λ³΄κ³ μ„
2. **251115_comprehensive_ml_keypoint_summary.md** - μΆ…ν•© ML μ›ν¬ν”λ΅μ°
3. **251103_success_report.md** - μ „μ²λ¦¬ κ°μ„  λ³΄κ³ μ„
4. **251104_silhouette_fitting_final.md** - Silhouette ν”Όν… λ³΄κ³ μ„

### μ™Έλ¶€ λ¦¬μ†μ¤
- **MAMMAL λ…Όλ¬Έ**: [Three-dimensional surface motion capture of multiple freely moving pigs using MAMMAL]
- **DANNCE**: https://github.com/spoonsso/dannce
- **PyTorch3D**: https://pytorch3d.org/
- **Ultralytics YOLOv8**: https://docs.ultralytics.com/
- **DeepLabCut**: https://deeplabcut.github.io/
- **Roboflow**: https://roboflow.com/

### ν”„λ΅μ νΈ νμ¤ν† λ¦¬
- **v1.0** (2025-11-03): κΈ°λ³Έ multi-view fitting, Hydra μ„¤μ •
- **v2.0** (2025-11-14~15):
  - Monocular fitting μ¶”κ°€
  - ML keypoint detection ν†µν•©
  - Manual labeling μ›ν¬ν”λ΅μ°
  - ν”„λ΅μ νΈ κµ¬μ΅° λ€ν­ μ •λ¦¬

---

## μ”μ•½: μ¶”μ² μ›ν¬ν”λ΅μ°

### μ΄λ³΄μ (λΉ λ¥Έ μ‹μ‘)
```bash
# 1. ν™κ²½ μ„¤μ •
bash setup.sh
conda activate mammal_stable

# 2. Monocular fitting μ‹λ„ (geometric)
python fit_monocular.py \
  --input_dir frames/ \
  --output_dir results/test \
  --detector geometric \
  --max_images 5

# 3. κ²°κ³Ό ν™•μΈ
ls results/test/
```

### μ¤‘κΈ‰μ (ν’μ§ ν–¥μƒ)
```bash
# 1. 20κ° μ΄λ―Έμ§€ μλ™ λΌλ²¨λ§ (Roboflow)
# 2. YOLO ν•™μµ
python scripts/train_yolo_pose.py --data data/...
# 3. Fine-tuned λ¨λΈλ΅ ν”Όν…
python fit_monocular.py --detector yolo --yolo_weights ...
```

### κ³ κΈ‰μ (Production)
```bash
# 1. μ»¤μ¤ν…€ λ°μ΄ν„°μ…‹ μ„¤μ •
# 2. Hyperparameter νλ‹
# 3. Multi-view + ML keypoint κ²°ν•©
# 4. Batch processing νμ΄ν”„λΌμΈ κµ¬μ¶•
```

---

**μµμΆ… μ—…λ°μ΄νΈ**: 2025-11-15
**μ‘μ„±μ**: MAMMAL Mouse Team
**λ¬Έμ**: docs/README.md μ°Έμ΅°
