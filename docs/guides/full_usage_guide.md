# MAMMAL_mouse

Three-dimensional surface motion capture of mice using the MAMMAL framework. This project enables markerless 3D pose estimation and mesh reconstruction for behavioral analysis by fitting an articulated 3D mouse model to video data.

![mouse_model](assets/figs/mouse_1.png)

---

## ðŸš€ Shell ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©ë²• (ê¶Œìž¥)

> **ê¶Œìž¥**: ì‰˜ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© ì‹œ `PYOPENGL_PLATFORM=egl` ìžë™ ì„¤ì •, í™˜ê²½ ê²€ì¦ í¬í•¨

### Multi-View Fitting (`run_mesh_fitting_default.sh`)

```bash
# ðŸŽ¯ Experiment ê¸°ë°˜ ì‹¤í–‰ (ê¶Œìž¥)
./run_mesh_fitting_default.sh quick_test           # conf/experiment/quick_test.yaml ì‚¬ìš©
./run_mesh_fitting_default.sh quick_test 0 5       # experiment + frame override

# ê¸°ë³¸ ì‚¬ìš© (frame 0-10, experiment ì—†ì´)
./run_mesh_fitting_default.sh - 0 10               # "-"ëŠ” experiment ìƒëžµ

# keypoint ì—†ì´ (silhouette only)
./run_mesh_fitting_default.sh - 0 10 -- --keypoints none

# ë‹¤ë¥¸ input_dir ì§€ì • + keypoint ì—†ì´
./run_mesh_fitting_default.sh - 0 10 -- --input_dir /home/joon/data/my_data --keypoints none
```

**Experiment configs** (`conf/experiment/`):

| Config | Views | Keypoints | ì„¤ëª… |
|--------|-------|-----------|------|
| `quick_test` | 6 | âœ… | 5 frames, ìµœì†Œ iterations (ë””ë²„ê¹…) |
| `views_6` | 6 | âœ… | Full baseline (100 samples) |
| `views_5` | 5 | âœ… | [0,1,2,3,4] |
| `views_4` | 4 | âœ… | [0,1,2,3] |
| `views_3_diagonal` | 3 | âœ… | [0,2,4] ëŒ€ê°ì„  ë°°ì¹˜ |
| `views_3_consecutive` | 3 | âœ… | [0,1,2] ì—°ì† ë°°ì¹˜ |
| `views_2_opposite` | 2 | âœ… | [0,3] ë°˜ëŒ€íŽ¸ |
| `views_1_single` | 1 | âœ… | [0] ë‹¨ì¼ë·° |
| `silhouette_only_6views` | 6 | âŒ | Maskë§Œ ì‚¬ìš© (keypoint ì—†ìŒ) |
| `silhouette_only_4views` | 4 | âŒ | |
| `silhouette_only_3views` | 3 | âŒ | |
| `silhouette_only_1view` | 1 | âŒ | |
| `accurate_6views` | 6 | âœ… | ê³ ì •ë°€ (iterations ì¦ê°€) |

**ë°ì´í„°ì…‹ ì •ë³´** (`data/examples/markerless_mouse_1_nerf/`):
| í•­ëª© | ê°’ |
|------|-----|
| ì¹´ë©”ë¼ | 6ê°œ (0~5) |
| ì´ í”„ë ˆìž„ | 18,000 frames |
| ê¸°ë³¸ ìƒ˜í”Œë§ | end_frame=1000, interval=10 â†’ 100 samples |

### Monocular Fitting (`run_mesh_fitting_monocular.sh`)

```bash
# ê¸°ë³¸ ì‚¬ìš© (ì „ì²´ ì´ë¯¸ì§€)
./run_mesh_fitting_monocular.sh data/frames/ results/monocular/output

# ì²˜ìŒ 10ê°œë§Œ
./run_mesh_fitting_monocular.sh data/frames/ results/monocular/output 10

# keypoint ì—†ì´ (silhouette only)
./run_mesh_fitting_monocular.sh data/frames/ results/monocular/output - -- --keypoints none

# ì²˜ìŒ 5ê°œ + silhouette only
./run_mesh_fitting_monocular.sh data/frames/ results/monocular/output 5 -- --keypoints none
```

**4x Upsampled ë°ì´í„°ì…‹** (`data/100-KO-male-56-20200615_4x/`):
```bash
# Cropped ì´ë¯¸ì§€ë¡œ silhouette-only fitting (ê¶Œìž¥)
./run_mesh_fitting_monocular.sh \
    data/100-KO-male-56-20200615_4x/cropped/ \
    results/monocular/shank3_4x/ \
    - -- --keypoints none

# ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
./run_mesh_fitting_monocular.sh \
    data/100-KO-male-56-20200615_4x/cropped/ \
    results/monocular/shank3_4x_test/ \
    5 -- --keypoints none
```

| í•­ëª© | ê°’ |
|------|-----|
| ê²½ë¡œ | `data/100-KO-male-56-20200615_4x/cropped/` |
| íŒŒì¼ íŒ¨í„´ | `*_cropped.png` + `*_mask.png` |
| í”„ë ˆìž„ ìˆ˜ | 20ê°œ |
| í•´ìƒë„ | ~516Ã—556 (4x upsampled) |

> **Note**: `--` ë’¤ì— ì¶”ê°€ ì¸ìžë¥¼ ì „ë‹¬í•˜ë©´ Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. EGL í™˜ê²½ë³€ìˆ˜ëŠ” ìžë™ ì„¤ì •ë©ë‹ˆë‹¤.

### ðŸ†• Silhouette-Only Fitting (keypoint ì—†ì´ ë§ˆìŠ¤í¬ë§Œ ì‚¬ìš©)

Keypoint annotation ì—†ì´ **mask silhouetteë§Œìœ¼ë¡œ** ë©”ì‹œ í”¼íŒ…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

#### ì›Œí¬í”Œë¡œìš°

```
1. ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸ (2 í”„ë ˆìž„) - ì˜¤ë¥˜ í™•ì¸
   â””â”€ ì„±ê³µ? â†’ 2. ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì„¤ì • ë¹„êµ
              â””â”€ ìµœì  ì„¤ì • í™•ì¸
                  â””â”€ 3. ì „ì²´ í”„ë ˆìž„ ì‹¤í–‰
```

#### Step 1: ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸ (í•„ìˆ˜!)

```bash
# âš ï¸ ì¤‘ìš”: --input_dirë¡œ ì‹¤ì œ ë°ì´í„° ê²½ë¡œ ì§€ì • (ì„œë²„ë§ˆë‹¤ ë‹¤ë¦„!)
./run_mesh_fitting_default.sh 0 2 -- --keypoints none \
    --input_dir /home/joon/MAMMAL_mouse/data/markerless_mouse_1_nerf

# ë°ì´í„° ìœ„ì¹˜ í™•ì¸
ls /home/joon/MAMMAL_mouse/data/
```

#### Step 2: ì‹¤í—˜ ë¹„êµ (ë””ë²„ê·¸ ì„±ê³µ í›„)

```bash
# 4ê°€ì§€ ì„¤ì •ìœ¼ë¡œ ìˆœì°¨ ì‹¤í—˜
./run_silhouette_experiments.sh /path/to/your/data 0 2
```

#### Step 3: ì „ì²´ ì‹¤í–‰

```bash
# ìµœì  ì„¤ì •ìœ¼ë¡œ ì „ì²´ í”„ë ˆìž„ ì‹¤í–‰
./run_mesh_fitting_default.sh 0 100 -- --keypoints none \
    --input_dir /path/to/data \
    silhouette.iter_multiplier=3.0 silhouette.theta_weight=15.0
```

**Silhouette ëª¨ë“œ ì„¤ì • ì˜µì…˜** (`conf/config.yaml` ë˜ëŠ” CLI):

| ì˜µì…˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `silhouette.iter_multiplier` | 2.0 | ë°˜ë³µ íšŸìˆ˜ ë°°ìœ¨ (ë†’ì„ìˆ˜ë¡ ì •ë°€) |
| `silhouette.theta_weight` | 10.0 | í¬ì¦ˆ ì •ê·œí™” (ë†’ì„ìˆ˜ë¡ ì•ˆì •ì ) |
| `silhouette.bone_weight` | 2.0 | ë¼ˆëŒ€ ê¸¸ì´ ì •ê·œí™” |
| `silhouette.scale_weight` | 50.0 | ìŠ¤ì¼€ì¼ ì •ê·œí™” |
| `silhouette.use_pca_init` | true | PCA ê¸°ë°˜ íšŒì „ ì´ˆê¸°í™” |

### ì„œë²„ ê°„ í˜¸í™˜ì„± (Portability)

ìŠ¤í¬ë¦½íŠ¸ì™€ configëŠ” ë‹¤ì–‘í•œ ì„œë²„ í™˜ê²½ì—ì„œ ë™ìž‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

**ìžë™ ì²˜ë¦¬ í•­ëª©:**
- Python ê²½ë¡œ: `miniconda3` / `anaconda3` ìžë™ ê°ì§€
- EGL í™˜ê²½ë³€ìˆ˜: ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìžë™ ì„¤ì •

**ìˆ˜ë™ ì§€ì • í•„ìš”:**
```bash
# ë°ì´í„° ê²½ë¡œëŠ” ì„œë²„ë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ í•­ìƒ --input_dir ì‚¬ìš©
./run_mesh_fitting_default.sh 0 10 -- --input_dir /your/server/data/path

# ë˜ëŠ” Hydra ë°©ì‹
python fitter_articulation.py data.data_dir=/your/server/data/path
```

**Config íŒŒì¼ì˜ ê²½ë¡œ:**
- ëª¨ë“  configëŠ” ìƒëŒ€ ê²½ë¡œ ì‚¬ìš© (`data/examples/...`)
- ì ˆëŒ€ ê²½ë¡œëŠ” CLIì—ì„œ override ê¶Œìž¥
- ì„œë²„ë³„ ë°ì´í„° ìœ„ì¹˜ í™•ì¸: `ls /home/$USER/*/data/`

---

## âš¡ Quick Start (5ë¶„ ì•ˆì— ì‹¤í–‰)

### ðŸ“ ë°ì´í„° ì¤€ë¹„

```bash
# ë°ì´í„° í´ë” êµ¬ì¡° (ì˜ˆì‹œ)
data/
â”œâ”€â”€ my_video/                    # Monocularìš© (ë‹¨ì¼ ì¹´ë©”ë¼)
â”‚   â”œâ”€â”€ 000000_rgb.png           # RGB ì´ë¯¸ì§€
â”‚   â”œâ”€â”€ 000000_mask.png          # ë°”ì´ë„ˆë¦¬ ë§ˆìŠ¤í¬
â”‚   â””â”€â”€ ...
â””â”€â”€ examples/markerless_mouse_1_nerf/   # Multi-viewìš© (ë‹¤ì¤‘ ì¹´ë©”ë¼)
    â”œâ”€â”€ videos_undist/           # 6ê°œ ë·° ë¹„ë””ì˜¤
    â”œâ”€â”€ simpleclick_undist/      # ë§ˆìŠ¤í¬
    â”œâ”€â”€ keypoints2d_undist/      # 2D keypoints
    â””â”€â”€ new_cam.pkl              # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°
```

### ðŸŽ¯ Monocular Fitting (ë‹¨ì¼ ì´ë¯¸ì§€/ë¹„ë””ì˜¤)

```bash
# í™˜ê²½ í™œì„±í™”
conda activate mammal_stable

# 1. ê¸°ë³¸ ì‹¤í–‰ (keypoint ê¸°ë°˜)
python fit_monocular.py \
    --input_dir data/my_video/ \
    --output_dir results/monocular/test/

# 2. Keypoint ì„ íƒ (ë¶€ì •í™•í•œ ë¶€ë¶„ ì œì™¸)
python fit_monocular.py \
    --input_dir data/my_video/ \
    --output_dir results/monocular/test/ \
    --keypoints spine,head      # head, spine, limbs, tail, centroid

# 3. Silhouette ê¸°ë°˜ (keypoint ì—†ì´ maskë§Œ ì‚¬ìš©)
python fit_monocular.py \
    --input_dir data/my_video/ \
    --output_dir results/monocular/test/ \
    --keypoints none            # mask IoU lossë¡œ fitting
```

**ì¶œë ¥ íŒŒì¼**:
```
results/monocular/test/
â”œâ”€â”€ *_mesh.obj          # 3D ë©”ì‹œ (Blender í˜¸í™˜)
â”œâ”€â”€ *_comparison.png    # RGB | Mask | Rendered | Overlay
â”œâ”€â”€ *_keypoints.png     # Keypoint ì‹œê°í™”
â”œâ”€â”€ *_rendered.png      # ë Œë”ë§ëœ mesh
â””â”€â”€ *_params.pkl        # MAMMAL íŒŒë¼ë¯¸í„°
```

### ðŸŽ¥ Multi-View Fitting (ë‹¤ì¤‘ ì¹´ë©”ë¼)

```bash
# í™˜ê²½ í™œì„±í™” (headless ì„œë²„ìš©)
conda activate mammal_stable
export PYOPENGL_PLATFORM=egl  # âš ï¸ í•„ìˆ˜! ì§ì ‘ python ì‹¤í–‰ ì‹œ ë°˜ë“œì‹œ ì„¤ì •

# 1. ê¸°ë³¸ ì‹¤í–‰ (Hydra ë°©ì‹)
python fitter_articulation.py \
    dataset=default_markerless \
    fitter.start_frame=0 \
    fitter.end_frame=10 

# 2. argparse ë°©ì‹ (fit_monocular.pyì™€ ë™ì¼í•œ CLI)
python fitter_articulation.py \
    --input_dir /path/to/data \
    --start_frame 0 \
    --end_frame 10 \
    --with_render

# 3. Keypoint ì—†ì´ Silhouetteë§Œ ì‚¬ìš©
python fitter_articulation.py \
    dataset=default_markerless \
    --keypoints none           # ë˜ëŠ” fitter.use_keypoints=false

# 4. í˜¼í•© ì‚¬ìš© (Hydra + argparse)
python fitter_articulation.py \
    dataset=default_markerless \
    --keypoints none \
    --with_render
```

> **CLI í˜¸í™˜ì„±**: `fitter_articulation.py`ëŠ” Hydra ë°©ì‹(`key=value`)ê³¼ argparse ë°©ì‹(`--key value`) ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.

**Config ì„¤ì •** (`conf/dataset/default_markerless.yaml`):
```yaml
video_dir: data/examples/markerless_mouse_1_nerf/videos_undist/
mask_dir: data/examples/markerless_mouse_1_nerf/simpleclick_undist/
keypoint_dir: data/examples/markerless_mouse_1_nerf/keypoints2d_undist/
cam_pkl: data/examples/markerless_mouse_1_nerf/new_cam.pkl
```

**ì¶œë ¥ ê²°ê³¼**:
```
results/fitting/{dataset}_{timestamp}/
â”œâ”€â”€ fitting_keypoints_*.png     # 6ë·° keypoint overlay
â”œâ”€â”€ render/fitting_*.png        # 6ë·° mesh rendering
â”œâ”€â”€ obj/*.obj                   # Frameë³„ 3D mesh
â””â”€â”€ params/*.pkl                # Frameë³„ íŒŒë¼ë¯¸í„°
```

### ðŸ“Š ê²°ê³¼ ì‹œê°í™”

```bash
# Cropped fitting ê²°ê³¼ + GT RGB ë¹„êµ
python scripts/utils/visualize_fitting_comparison.py \
    --results results/cropped_fitting_final \
    --gt_dir data/cropped_images \
    --output results/gallery.png
```

---

## ðŸ“ ê²°ê³¼ ì¶œë ¥ë¬¼ ê°€ì´ë“œ (Output Structure)

ì‹¤í—˜ ì™„ë£Œ í›„ `results/fitting/{experiment_name}/` í´ë”ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

### í´ë” êµ¬ì¡°
```
results/fitting/{dataset}_{views}_{keypoints}_{timestamp}/
â”œâ”€â”€ config.yaml                              # ì‹¤í—˜ ì„¤ì • (ìž¬í˜„ìš©)
â”œâ”€â”€ loss_history.json                        # í•™ìŠµ ë¡œìŠ¤ ê¸°ë¡
â”œâ”€â”€ render/
â”‚   â”œâ”€â”€ step_1_frame_000000.png              # Step1 ê²°ê³¼ + í‚¤í¬ì¸íŠ¸ ì˜¤ë²„ë ˆì´
â”‚   â”œâ”€â”€ step_2_frame_000000.png              # Step2 ìµœì¢… ê²°ê³¼
â”‚   â”œâ”€â”€ step_summary_frame_000000.png        # 3ë‹¨ê³„ ë¹„êµ (ì²« í”„ë ˆìž„)
â”‚   â”œâ”€â”€ debug/                               # ì¤‘ê°„ iteration ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ step_0_frame_000000_iter_00000.png
â”‚   â”‚   â”œâ”€â”€ step_1_frame_000000_iter_00000.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ keypoints/                           # GT vs Predicted ë¹„êµ
â”‚       â”œâ”€â”€ step_1_frame_000000_keypoints.png
â”‚       â”œâ”€â”€ step_1_frame_000000_keypoints_gt.png
â”‚       â””â”€â”€ step_1_frame_000000_keypoints_compare.png
â”œâ”€â”€ params/                                  # ëª¨ë¸ íŒŒë¼ë¯¸í„° (pickle)
â”‚   â”œâ”€â”€ step_1_frame_000000.pkl
â”‚   â””â”€â”€ step_2_frame_000000.pkl
â””â”€â”€ obj/                                     # 3D ë©”ì‹œ íŒŒì¼
    â””â”€â”€ step_2_frame_000000.obj
```

### Fitting 3ë‹¨ê³„ ì„¤ëª…

| Step | ì´ë¦„ | ìµœì í™” ëŒ€ìƒ | ì„¤ëª… |
|------|------|------------|------|
| **Step 0** | Global Positioning | `trans`, `rotation`, `scale` | ì´ˆê¸° ìœ„ì¹˜/í¬ê¸°/ë°©í–¥ ì„¤ì • (ê´€ì ˆê° ê³ ì •) |
| **Step 1** | Articulation Fitting | `thetas`, `bone_lengths` | ê´€ì ˆ ê°ë„ì™€ ë¼ˆ ê¸¸ì´ ìµœì í™” (í¬ì¦ˆ í”¼íŒ… í•µì‹¬) |
| **Step 2** | Silhouette Refinement | ì „ì²´ íŒŒë¼ë¯¸í„° | Mask loss í™œì„±í™”, ì‹¤ë£¨ì—£ ì •êµí™” |

### ðŸŽ¯ 3D Geometric Priorë¡œ ì‚¬ìš©í•˜ê¸°

ë‹¤ë¥¸ í”„ë¡œì íŠ¸(ì˜ˆ: NeRF, 3D Gaussian Splatting)ì—ì„œ í”„ë ˆìž„ë³„ 3D meshë¥¼ geometric priorë¡œ í™œìš©í•˜ë ¤ë©´:

#### 1. OBJ íŒŒì¼ (ê¶Œìž¥ - ê°€ìž¥ ê°„ë‹¨)
```python
# í”„ë ˆìž„ë³„ 3D ë©”ì‹œ ì§ì ‘ ë¡œë“œ
import trimesh

mesh = trimesh.load("results/fitting/.../obj/step_2_frame_000000.obj")
vertices = mesh.vertices  # (N_verts, 3) - 3D ì¢Œí‘œ
faces = mesh.faces        # (N_faces, 3) - ì‚¼ê°í˜• ì¸ë±ìŠ¤

# ëª¨ë“  í”„ë ˆìž„ ë¡œë“œ
import glob
obj_files = sorted(glob.glob("results/fitting/.../obj/step_2_frame_*.obj"))
meshes = [trimesh.load(f) for f in obj_files]
```

#### 2. PKL íŒŒì¼ (íŒŒë¼ë¯¸í„° ìž¬ì‚¬ìš© - ê³ ê¸‰)
```python
import pickle
import torch

# íŒŒë¼ë¯¸í„° ë¡œë“œ
with open("results/fitting/.../params/step_2_frame_000000.pkl", "rb") as f:
    params = pickle.load(f)

# params êµ¬ì¡°:
# {
#     "thetas": (1, 20, 3),       # ê´€ì ˆ íšŒì „ (axis-angle)
#     "bone_lengths": (1, 20),    # ë¼ˆ ê¸¸ì´ ì˜¤í”„ì…‹
#     "trans": (1, 3),            # 3D ìœ„ì¹˜ (mm)
#     "rotation": (1, 3),         # ì „ì—­ íšŒì „ (axis-angle)
#     "scale": (1, 1),            # ìŠ¤ì¼€ì¼ íŒ©í„°
#     "chest_deformer": (1, 1),   # ê°€ìŠ´ ë³€í˜•
# }

# BodyModelë¡œ ë©”ì‹œ ìž¬ìƒì„±
from bodymodel_th import BodyModelTorch
bodymodel = BodyModelTorch(device='cuda')
V, J = bodymodel.forward(
    params["thetas"], params["bone_lengths"],
    params["rotation"], params["trans"], params["scale"],
    params["chest_deformer"]
)
vertices = V[0].cpu().numpy()  # (N_verts, 3)
```

#### 3. ë©€í‹°ë·° ë°ì´í„° êµ¬ì¡°

```
data/examples/markerless_mouse_1_nerf/
â”œâ”€â”€ videos_undist/
â”‚   â”œâ”€â”€ 0.mp4          # View 0 (ì¹´ë©”ë¼ 0)
â”‚   â”œâ”€â”€ 1.mp4          # View 1 (ì¹´ë©”ë¼ 1)
â”‚   â”œâ”€â”€ 2.mp4          # View 2 (ì¹´ë©”ë¼ 2)
â”‚   â”œâ”€â”€ 3.mp4          # View 3 (ì¹´ë©”ë¼ 3)
â”‚   â”œâ”€â”€ 4.mp4          # View 4 (ì¹´ë©”ë¼ 4)
â”‚   â””â”€â”€ 5.mp4          # View 5 (ì¹´ë©”ë¼ 5)
â”œâ”€â”€ new_cam.pkl        # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° (6ê°œ ì¹´ë©”ë¼)
â””â”€â”€ keypoints2d_undist/
    â”œâ”€â”€ result_view_0.pkl  # View 0 2D í‚¤í¬ì¸íŠ¸
    â””â”€â”€ ...
```

**ë·° ì‹ë³„ ë°©ë²•:**
- ë¹„ë””ì˜¤ íŒŒì¼ëª… = ì¹´ë©”ë¼ ID (ì˜ˆ: `0.mp4` â†’ Camera 0)
- ë™ì¼ í”„ë ˆìž„ ì¸ë±ìŠ¤ = ë™ì¼ ì‹œì  (ëª¨ë“  ì¹´ë©”ë¼ ë™ê¸°í™”ë¨)
- `new_cam.pkl`: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ, `cams[i]`ê°€ Camera iì˜ íŒŒë¼ë¯¸í„°

**ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° êµ¬ì¡°:**
```python
import pickle
with open("new_cam.pkl", "rb") as f:
    cams = pickle.load(f)  # List[Dict]

# cams[i] êµ¬ì¡°:
# {
#     'K': (3, 3),    # Intrinsic matrix
#     'R': (3, 3),    # Rotation matrix
#     'T': (3, 1),    # Translation vector
#     'mapx': ...,    # Undistortion map x
#     'mapy': ...,    # Undistortion map y
# }
```

**ë©€í‹°ë·° ë™ê¸°í™”:**
- ë™ê¸°í™” ê¸°ì¤€: í”„ë ˆìž„ ì¸ë±ìŠ¤ (íŒŒì¼ëª…ì˜ `frame_XXXXXX`)
- ê°€ì •: ëª¨ë“  ì¹´ë©”ë¼ê°€ ë™ê¸°í™”ëœ ë…¹í™” (ë™ì¼ FPS, ë™ì¼ ì‹œìž‘ì )
- Fitting ì‹œ ë™ì¼ frame indexë¡œ ëª¨ë“  ë·° ë™ì‹œ ì ‘ê·¼

### í™œìš© ì˜ˆì‹œ

```python
# ì˜ˆ: 4D-GSì—ì„œ í”„ë ˆìž„ë³„ meshë¥¼ deformation priorë¡œ ì‚¬ìš©
for frame_idx in range(num_frames):
    mesh = trimesh.load(f"obj/step_2_frame_{frame_idx:06d}.obj")

    # Mesh verticesë¥¼ Gaussian ì´ˆê¸°í™”ì— ì‚¬ìš©
    init_positions = mesh.vertices
    init_normals = mesh.vertex_normals

    # ë˜ëŠ” mesh surfaceì—ì„œ ìƒ˜í”Œë§
    points, face_indices = trimesh.sample.sample_surface(mesh, count=10000)
```

### 3D ë©”ì‹œ ì‹œí€€ìŠ¤ ì‹œê°í™” ë° ì˜ìƒ ì €ìž¥

```bash
# PKLì—ì„œ ë©”ì‹œ ìž¬êµ¬ì„±í•˜ì—¬ ì˜ìƒ ì €ìž¥ (BodyModel í•„ìš”)
python scripts/visualize_mesh_sequence.py results/fitting/xxx --output mesh_sequence.mp4

# OBJ íŒŒì¼ ì§ì ‘ ì‚¬ìš© (BodyModel ì—†ì´ ë…ë¦½ ì‹¤í–‰)
python scripts/visualize_mesh_sequence.py results/fitting/xxx --use-obj --output mesh.mp4

# íŠ¹ì • ë·°í¬ì¸íŠ¸ì—ì„œ ë Œë”ë§
python scripts/visualize_mesh_sequence.py results/fitting/xxx \
    --azimuth 45 --elevation 30 --output side_view.mp4

# 360Â° íšŒì „ ë·° ìƒì„±
python scripts/visualize_mesh_sequence.py results/fitting/xxx --rotating --output rotating.mp4

# Pyrender ì‚¬ìš© (ë” ê³ í’ˆì§ˆ, EGL í•„ìš”)
python scripts/visualize_mesh_sequence.py results/fitting/xxx --use-pyrender --output hq.mp4
```

**ì˜µì…˜:**
| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--use-obj` | OBJ íŒŒì¼ ì§ì ‘ ë¡œë“œ (BodyModel ë¶ˆí•„ìš”) | False |
| `--azimuth` | ì¹´ë©”ë¼ ë°©ìœ„ê° (ë„) | 45 |
| `--elevation` | ì¹´ë©”ë¼ ê³ ë„ê° (ë„) | 30 |
| `--rotating` | í”„ë ˆìž„ë³„ 360Â° íšŒì „ | False |
| `--fps` | ì¶œë ¥ ì˜ìƒ FPS | 30 |
| `--use-pyrender` | Pyrender ë Œë”ëŸ¬ ì‚¬ìš© | False |

---

## âœ¨ Features

- **Multi-view 3D fitting**: Fit 3D mouse model to synchronized multi-camera videos
- **Single-view (monocular) fitting**: Process single videos with ML-based keypoint detection
- **ML keypoint detection**: YOLOv8-Pose and SuperAnimal support for anatomically accurate keypoints
- **ðŸ†• Flexible keypoint annotation**: Manual annotation tool + automatic format conversion (1-22 keypoints)
- **Confidence-based filtering**: Missing keypoints automatically ignored (no need for all 22!)
- **Hydra configuration**: Flexible experiment management with dataset-specific configs
- **Modular pipeline**: Separate preprocessing and fitting stages for easy customization

---

## ðŸš€ Quick Start

### Step 1: Clone and Install

```bash
# Clone the repository
git clone https://github.com/your-username/MAMMAL_mouse.git
cd MAMMAL_mouse

# Create conda environment and install dependencies (one-time setup)
bash scripts/setup/setup.sh
```

**What this installs**:
- Python 3.10 environment named `mammal_stable`
- PyTorch 2.0.0 + CUDA 11.8
- PyTorch3D 0.7.5
- All required dependencies (opencv, hydra, ultralytics, etc.)

**Requirements**:
- Anaconda/Miniconda
- NVIDIA GPU with CUDA 11.8
- ~10GB disk space for dependencies

### Step 2: Download Data and Models

#### Option A: Example Dataset (Recommended for First-Time Users)

Download the example multi-view dataset:

```bash
# Download from Google Drive
# https://drive.google.com/file/d/1NbaIFOvpvQ_WLOabUtMrVHS7vVBq-8zD/view?usp=sharing

# Extract to the correct location
unzip markerless_mouse_1_nerf.zip
mv markerless_mouse_1_nerf/ data/examples/
```

**Dataset structure**:
```
data/examples/markerless_mouse_1_nerf/
â”œâ”€â”€ videos_undist/           # 6 camera views
â”‚   â”œâ”€â”€ 0.mp4
â”‚   â”œâ”€â”€ 1.mp4
â”‚   â””â”€â”€ ...
â”œâ”€â”€ simpleclick_undist/      # Binary masks
â”‚   â”œâ”€â”€ 0.mp4
â”‚   â””â”€â”€ ...
â”œâ”€â”€ keypoints2d_undist/      # 2D keypoints
â”‚   â”œâ”€â”€ result_view_0.pkl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ new_cam.pkl              # Camera parameters
â””â”€â”€ new_params.pkl           # Model parameters
```

#### Option B: Your Own Video

If you have your own single-view video:

```bash
# 1. Place your video
mkdir -p data/raw/my_experiment/
cp /path/to/your/video.mp4 data/raw/my_experiment/

# 2. Extract frames (optional, for monocular fitting)
mkdir -p data/raw/my_experiment/frames/
ffmpeg -i data/raw/my_experiment/video.mp4 \
  -vf "fps=30" data/raw/my_experiment/frames/%06d.png
```

#### Optional: Download Pretrained Models

For ML-based keypoint detection:

```bash
# YOLOv8-Pose pretrained model (auto-downloaded on first use)
# Will be saved to: models/pretrained/yolov8n-pose.pt

# SuperAnimal-TopViewMouse model (optional, 245MB)
python scripts/setup/download_superanimal.py
# Saved to: models/pretrained/superanimal_topviewmouse/
```

### Step 3: Run Your First Experiment

#### Scenario 1: Multi-View Fitting (Example Dataset) â­ Recommended for Testing

Process the example multi-view dataset:

```bash
# Activate environment
conda activate mammal_stable

# Run fitting on first 10 frames (using shell script)
./run_mesh_fitting_default.sh 0 10

# OR run directly with Python
python fitter_articulation.py \
  dataset=default_markerless \
  fitter.start_frame=0 \
  fitter.end_frame=10 \
  fitter.with_render=true
```

**What happens**:
1. Loads 6-camera preprocessed data from `data/examples/markerless_mouse_1_nerf/`
2. Fits 3D mouse model to frames 0-10
3. Saves results to `results/fitting/{dataset}_{timestamp}/`

**Expected output**:
```
results/fitting/markerless_mouse_1_nerf_20251125_143000/
â”œâ”€â”€ obj/                     # 3D mesh files (.obj)
â”‚   â”œâ”€â”€ mesh_000000.obj
â”‚   â”œâ”€â”€ mesh_000002.obj
â”‚   â””â”€â”€ ...
â”œâ”€â”€ params/                  # Fitting parameters (.pkl)
â”‚   â”œâ”€â”€ param0.pkl
â”‚   â”œâ”€â”€ param0_sil.pkl
â”‚   â””â”€â”€ ...
â””â”€â”€ render/                  # Visualization overlays (.png)
    â”œâ”€â”€ fitting_0.png
    â”œâ”€â”€ fitting_0_sil.png
    â””â”€â”€ debug/               # Optimization debug images
```

**Processing time**: ~5-10 minutes (RTX 3090)

#### Scenario 2: Monocular Fitting (Single Video) ðŸ†•

Process a single-view video with ML keypoint detection:

```bash
conda activate mammal_stable

# Using geometric keypoint detection (baseline)
python fit_monocular.py \
  --input_dir data/raw/my_experiment/frames/ \
  --output_dir results/monocular/my_experiment \
  --detector geometric \
  --max_images 10

# OR using YOLOv8-Pose (better quality, requires GPU)
python fit_monocular.py \
  --input_dir data/raw/my_experiment/frames/ \
  --output_dir results/monocular/my_experiment \
  --detector yolo \
  --max_images 10
```

**What happens**:
1. Detects 22 keypoints per frame using chosen detector
2. Estimates camera parameters from first frame
3. Fits 3D mouse model frame-by-frame
4. Saves meshes, parameters, and visualizations

**Expected output**:
```
results/monocular/my_experiment/
â”œâ”€â”€ obj/                     # 3D mesh files
â”‚   â”œâ”€â”€ frame_000001.obj
â”‚   â””â”€â”€ ...
â”œâ”€â”€ params/                  # Fitting parameters
â”‚   â”œâ”€â”€ frame_000001.pkl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ keypoints_2d/            # Detected 2D keypoints
â”‚   â”œâ”€â”€ frame_000001.pkl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ camera_params.pkl        # Estimated camera
â””â”€â”€ visualizations/          # Overlays (if enabled)
```

**Processing time**: ~30 seconds/frame (geometric), ~1 minute/frame (YOLO)

#### Scenario 3: Traditional Preprocessing + Fitting

Full pipeline for single-view video:

```bash
conda activate mammal_stable

# 1. Preprocess video (extract frames, masks, keypoints)
python scripts/preprocess.py \
  dataset=custom \
  mode=single_view_preprocess \
  preprocess.input_video_path="data/raw/my_experiment/video.mp4" \
  preprocess.output_data_dir="data/preprocessed/my_experiment/"

# 2. Fit 3D model to preprocessed data
python fitter_articulation.py \
  dataset=custom \
  data.data_dir="data/preprocessed/my_experiment/" \
  fitter.end_frame=100 \
  fitter.with_render=false
```

**Preprocessing outputs**:
```
data/preprocessed/my_experiment/
â”œâ”€â”€ videos_undist/
â”‚   â””â”€â”€ 0.mp4                # Original video
â”œâ”€â”€ simpleclick_undist/
â”‚   â””â”€â”€ 0.mp4                # Binary mask video
â”œâ”€â”€ keypoints2d_undist/
â”‚   â””â”€â”€ result_view_0.pkl    # 22 keypoints per frame
â””â”€â”€ new_cam.pkl              # Camera parameters
```

---

## ðŸ“– Usage Scenarios

### 1ï¸âƒ£ Quick Test with Example Data (5 minutes)

**Goal**: Verify installation and see multi-view fitting results

```bash
conda activate mammal_stable

# Using shell script (recommended)
./run_mesh_fitting_default.sh 0 5

# OR using Python directly
python fitter_articulation.py \
  dataset=default_markerless \
  optim=fast \
  fitter.end_frame=5
```

**Results**: `results/fitting/{dataset}_{timestamp}/obj/` contains 3D meshes

### 2ï¸âƒ£ Process Your Single Video (30 minutes)

**Goal**: Get 3D pose from your own video

```bash
conda activate mammal_stable

# Extract frames from video
mkdir -p data/raw/my_video/frames/
ffmpeg -i your_video.mp4 data/raw/my_video/frames/%06d.png

# Using shell script (recommended)
./run_mesh_fitting_monocular.sh data/raw/my_video/frames/ results/monocular/my_video yolo

# OR using Python directly
python fit_monocular.py \
  --input_dir data/raw/my_video/frames/ \
  --output_dir results/monocular/my_video \
  --detector yolo \
  --max_images 50
```

**Results**: `results/monocular/my_video/obj/` contains 3D meshes

### 3ï¸âƒ£ Train Custom ML Detector (1 day)

**Goal**: Improve keypoint detection for your specific setup

**Step 1: Sample images** (5 min)
```bash
conda activate mammal_stable

python scripts/setup/sample_images_for_labeling.py \
  --input_dir data/raw/my_video/frames/ \
  --output_dir data/training/manual_labeling/images/ \
  --num_samples 20
```

**Step 2: Label on Roboflow** (2-3 hours)
1. Create account at https://roboflow.com
2. Create new "Keypoint Detection" project
3. Define 22 keypoints (see `docs/guides/ROBOFLOW_LABELING_GUIDE.md`)
4. Upload 20 images and label all keypoints
5. Export as "YOLOv8 Pose" format

**Step 3: Train YOLOv8** (30 min)
```bash
conda activate mammal_stable

# Merge manual labels with geometric labels
python preprocessing_utils/merge_datasets.py \
  --manual data/training/manual_labeling/ \
  --geometric data/training/yolo_mouse_pose/ \
  --output data/training/yolo_enhanced/

# Train YOLOv8
python scripts/train_yolo_pose.py \
  --data data/training/yolo_enhanced/data.yaml \
  --epochs 100 \
  --batch 8 \
  --imgsz 256 \
  --name my_custom_detector
```

**Step 4: Use trained model**
```bash
python fit_monocular.py \
  --detector yolo \
  --yolo_weights models/trained/yolo/my_custom_detector/weights/best.pt
```

**Expected improvements**:
- Confidence: 0.5 â†’ 0.85+ (2Ã—)
- Loss: ~300K â†’ 15-30K (10-20Ã—)
- Paw detection: 0% â†’ 70-80%

### 4ï¸âƒ£ Batch Process Multiple Videos (customizable)

Process a directory of videos:

```bash
conda activate mammal_stable

# Create batch processing script
cat > batch_process.sh << 'EOF'
#!/bin/bash
for video in data/raw/batch/*.mp4; do
  name=$(basename "$video" .mp4)
  echo "Processing $name..."

  # Extract frames
  mkdir -p "data/raw/batch/${name}_frames/"
  ffmpeg -i "$video" "data/raw/batch/${name}_frames/%06d.png"

  # Run monocular fitting
  python fit_monocular.py \
    --input_dir "data/raw/batch/${name}_frames/" \
    --output_dir "results/monocular/${name}/" \
    --detector yolo \
    --max_images 100
done
EOF

chmod +x batch_process.sh
./batch_process.sh
```

---

## ðŸ“Š Understanding the Output

### Multi-View Fitting Output

After running `fitter_articulation.py`, outputs are in `results/fitting/{dataset}_{timestamp}/`:

```
results/fitting/markerless_mouse_1_nerf_20251125_143000/
â”œâ”€â”€ obj/                           # 3D mesh files (can open in Blender/MeshLab)
â”‚   â”œâ”€â”€ mesh_000000.obj            # Mesh for frame 0
â”‚   â”œâ”€â”€ mesh_000002.obj
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ params/                        # Fitting parameters (Python pickle)
â”‚   â”œâ”€â”€ param0.pkl                 # Contains: body_pose, global_orient, betas, etc.
â”‚   â”œâ”€â”€ param0_sil.pkl             # After silhouette refinement
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ render/                        # Visualization overlays (if with_render=true)
â”‚   â”œâ”€â”€ fitting_0.png              # Fitted model overlaid on all views
â”‚   â”œâ”€â”€ fitting_0_sil.png          # After silhouette refinement
â”‚   â””â”€â”€ debug/                     # Optimization debug images
â”‚
â””â”€â”€ .hydra/                        # Hydra config snapshots
    â””â”€â”€ config.yaml                # Exact config used for this run
```

**How to visualize**:
```bash
# View 3D mesh in Blender
blender results/fitting/*/obj/mesh_000000.obj

# View 3D mesh in MeshLab
meshlab results/fitting/*/obj/mesh_000000.obj

# View overlays
eog results/fitting/*/render/fitting_0.png
```

### Monocular Fitting Output

After running `fit_monocular.py`, outputs are in specified `--output_dir`:

```
results/monocular/my_experiment/
â”œâ”€â”€ obj/                           # 3D mesh files
â”‚   â”œâ”€â”€ frame_000001.obj
â”‚   â”œâ”€â”€ frame_000002.obj
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ params/                        # Fitting parameters
â”‚   â”œâ”€â”€ frame_000001.pkl
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ keypoints_2d/                  # Detected 2D keypoints
â”‚   â”œâ”€â”€ frame_000001.pkl           # 22 keypoints [x, y, conf]
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ camera_params.pkl              # Estimated camera intrinsics
â”‚
â””â”€â”€ visualizations/                # Overlays (if --visualize)
    â”œâ”€â”€ frame_000001.png           # Keypoints overlaid on image
    â””â”€â”€ ...
```

**How to inspect keypoints**:
```python
import pickle
import numpy as np

# Load keypoints for frame 1
with open('results/monocular/my_experiment/keypoints_2d/frame_000001.pkl', 'rb') as f:
    kpts = pickle.load(f)

print(kpts.shape)  # (22, 3) -> [x, y, confidence]
# Note: GT annotationì—ì„œ noseëŠ” index 2 (mouse_22_defs.py ê¸°ì¤€)
# Model outputì—ì„œ noseëŠ” index 0 (keypoint22_mapper.json ê¸°ì¤€)
print(f"Nose (model idx 0): x={kpts[0,0]:.1f}, y={kpts[0,1]:.1f}, conf={kpts[0,2]:.2f}")
```

**How to inspect fitted parameters**:
```python
import pickle

# Load fitted parameters
with open('results/monocular/my_experiment/params/frame_000001.pkl', 'rb') as f:
    params = pickle.load(f)

print(params.keys())
# dict_keys(['body_pose', 'global_orient', 'betas', 'transl'])

print(f"Body pose shape: {params['body_pose'].shape}")  # Limb rotations
print(f"Global orient shape: {params['global_orient'].shape}")  # Root orientation
print(f"Translation: {params['transl']}")  # 3D position
```

### Training Output

After running `scripts/train_yolo_pose.py`, training results are in `models/trained/yolo/`:

```
models/trained/yolo/my_custom_detector/
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ best.pt                    # Best model checkpoint (use this!)
â”‚   â””â”€â”€ last.pt                    # Last epoch checkpoint
â”‚
â”œâ”€â”€ results.png                    # Training curves (loss, mAP, etc.)
â”œâ”€â”€ confusion_matrix.png           # Confusion matrix
â”œâ”€â”€ PR_curve.png                   # Precision-Recall curve
â”œâ”€â”€ results.csv                    # Metrics per epoch
â””â”€â”€ args.yaml                      # Training arguments
```

**How to evaluate**:
```bash
# View training curves
eog models/trained/yolo/my_custom_detector/results.png

# Check final metrics
tail -1 models/trained/yolo/my_custom_detector/results.csv
```

---

## âš™ï¸ Configuration Guide

### Hydra Configuration System

This project uses [Hydra](https://hydra.cc/) for flexible configuration. Config files are in `conf/`:

```
conf/
â”œâ”€â”€ config.yaml              # Main config (don't edit directly)
â”œâ”€â”€ dataset/                 # Dataset-specific configs
â”‚   â”œâ”€â”€ markerless.yaml      # Multi-view (6 cameras)
â”‚   â”œâ”€â”€ shank3.yaml          # Single-view
â”‚   â””â”€â”€ custom.yaml          # Template for your data
â”œâ”€â”€ preprocess/              # Preprocessing configs
â”‚   â”œâ”€â”€ opencv.yaml          # Current: geometric keypoints
â”‚   â””â”€â”€ sam.yaml             # Future: SAM-based masking
â””â”€â”€ optim/                   # Optimization configs
    â”œâ”€â”€ fast.yaml            # Quick test (fewer iterations)
    â””â”€â”€ accurate.yaml        # High quality (more iterations)
```

### Key Parameters

| Parameter | Description | Default | Example |
|-----------|-------------|---------|---------|
| `dataset` | Which dataset config to use | `shank3` | `dataset=markerless` |
| `optim` | Optimization settings | `fast` | `optim=accurate` |
| `mode` | Processing mode | `multi_view` | `mode=single_view_preprocess` |
| `data.data_dir` | Input data path | varies | `data.data_dir="data/preprocessed/custom/"` |
| `fitter.start_frame` | First frame | `0` | `fitter.start_frame=10` |
| `fitter.end_frame` | Last frame | `2` | `fitter.end_frame=100` |
| `fitter.with_render` | Enable rendering | `false` | `fitter.with_render=true` |
| `fitter.use_keypoints` | Enable keypoint loss | `true` | `fitter.use_keypoints=false` |
| `optim.solve_step0_iters` | Step 0 iterations | `10` | `optim.solve_step0_iters=20` |
| `optim.solve_step1_iters` | Step 1 iterations | `100` | `optim.solve_step1_iters=200` |
| `optim.solve_step2_iters` | Step 2 iterations | `30` | `optim.solve_step2_iters=50` |

### CLI ì¸ìž â†” Hydra ë§¤í•‘

`fitter_articulation.py`ëŠ” argparse ìŠ¤íƒ€ì¼ ì¸ìžë¥¼ ìžë™ìœ¼ë¡œ Hydra í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

| argparse ìŠ¤íƒ€ì¼ | Hydra í˜•ì‹ |
|----------------|-----------|
| `--keypoints none` | `fitter.use_keypoints=false` |
| `--input_dir /path` | `data.data_dir=/path` |
| `--output_dir /path` | `result_folder=/path` |
| `--start_frame N` | `fitter.start_frame=N` |
| `--end_frame N` | `fitter.end_frame=N` |
| `--with_render` | `fitter.with_render=true` |

### ðŸ†• Manual Keypoint Annotation Workflow

For detailed mesh fitting with custom keypoint annotations:

**Quick workflow**:
```bash
# 1. Annotate keypoints (Gradio UI)
python keypoint_annotator_v2.py data/100-KO-male-56-20200615_cropped

# 2. Convert to MAMMAL format
python convert_keypoints_to_mammal.py \
  --input keypoints.json \
  --output data/.../keypoints2d_undist/result_view_0.pkl \
  --num-frames 20

# 3. Run mesh fitting
python fitter_articulation.py dataset=custom_cropped
```

**Key features**:
- âœ… **Flexible keypoint count**: 1-22 keypoints (recomm 5-7)
- âœ… **Auto-filtering**: Missing keypoints ignored automatically
- âœ… **Interactive UI**: Zoom, visibility control, progress tracking

ðŸ“– **Full guide**: [`KEYPOINT_QUICK_START.md`](KEYPOINT_QUICK_START.md) | [`docs/KEYPOINT_WORKFLOW.md`](docs/KEYPOINT_WORKFLOW.md)

---

### Usage Examples

```bash
# Use markerless dataset with accurate optimization
python fitter_articulation.py \
  dataset=markerless \
  optim=accurate

# Process frames 50-100 with rendering
python fitter_articulation.py \
  dataset=markerless \
  fitter.start_frame=50 \
  fitter.end_frame=100 \
  fitter.with_render=true

# Quick test on first 5 frames
python fitter_articulation.py \
  dataset=markerless \
  optim=fast \
  fitter.end_frame=5

# Override data directory
python fitter_articulation.py \
  data.data_dir="data/preprocessed/custom/" \
  fitter.end_frame=10
```

### Creating Custom Dataset Config

1. Copy template:
```bash
cp conf/dataset/custom.yaml conf/dataset/my_dataset.yaml
```

2. Edit `conf/dataset/my_dataset.yaml`:
```yaml
# @package _global_

data:
  data_dir: "data/preprocessed/my_dataset/"
  num_views: 1  # Single camera

fitter:
  start_frame: 0
  end_frame: 100

preprocess:
  input_video_path: "data/raw/my_dataset/video.mp4"
  output_data_dir: "data/preprocessed/my_dataset/"
```

3. Use it:
```bash
python fitter_articulation.py dataset=my_dataset
```

---

## ðŸ”¬ Advanced Usage

### Custom Keypoint Order

The default keypoint order is defined in `mouse_22_defs.py`:

```python
# 22 anatomical keypoints
KEYPOINT_NAMES = [
    'nose', 'left_ear', 'right_ear', 'left_eye', 'right_eye',
    'head_center', 'spine_1', 'spine_2', 'spine_3', 'spine_4',
    'spine_5', 'spine_6', 'spine_7', 'spine_8',
    'left_paw_front', 'right_paw_front',
    'left_paw_rear', 'right_paw_rear',
    'tail_base', 'tail_mid', 'tail_tip', 'centroid'
]
```

To use different keypoints, modify `mouse_22_defs.py` and update detection accordingly.

### Batch Processing with GNU Parallel

Process multiple datasets in parallel:

```bash
# Install GNU parallel
sudo apt-get install parallel

# Create experiment list
cat > experiments.txt << EOF
markerless 0 10
markerless 10 20
markerless 20 30
EOF

# Run in parallel (4 jobs)
parallel -j 4 --colsep ' ' \
  python fitter_articulation.py \
  dataset={1} \
  fitter.start_frame={2} \
  fitter.end_frame={3} \
  :::: experiments.txt
```

### Exporting Results

Convert 3D meshes to different formats:

```bash
# Convert OBJ to PLY
conda activate mammal_stable
pip install trimesh

python << EOF
import trimesh
import glob

for obj_file in glob.glob('results/fitting/*/obj/*.obj'):
    mesh = trimesh.load(obj_file)
    ply_file = obj_file.replace('.obj', '.ply')
    mesh.export(ply_file)
    print(f"Converted {obj_file} -> {ply_file}")
EOF
```

### Visualization with PyVista

Interactive 3D visualization:

```bash
conda activate mammal_stable
pip install pyvista

python << EOF
import pyvista as pv
import glob

# Load all meshes
meshes = []
for obj_file in sorted(glob.glob('results/fitting/*/obj/mesh_*.obj')):
    meshes.append(pv.read(obj_file))

# Create animation
plotter = pv.Plotter()
for mesh in meshes:
    plotter.add_mesh(mesh, color='tan')
    plotter.show(auto_close=False)
    plotter.clear()
EOF
```

---

## ðŸ”§ Troubleshooting

### Installation Issues

**Problem**: `bash scripts/setup/setup.sh` fails
```bash
# Solution 1: Check conda is installed
conda --version

# Solution 2: Update conda
conda update -n base -c defaults conda

# Solution 3: Manual installation
conda create -n mammal_stable python=3.10 -y
conda activate mammal_stable
conda install pytorch==2.0.0 torchvision==0.15.0 pytorch-cuda=11.8 -c pytorch -c nvidia -y
pip install -r requirements.txt
```

**Problem**: `CUDA out of memory`
```bash
# Solution: Reduce batch size or process fewer frames
python fitter_articulation.py fitter.end_frame=5  # Instead of 10
python fit_monocular.py --max_images 5  # Instead of 10
```

**Problem**: `ModuleNotFoundError: No module named 'pytorch3d'`
```bash
# Solution: Reinstall pytorch3d
conda activate mammal_stable
pip uninstall pytorch3d -y
pip install "git+https://github.com/facebookresearch/pytorch3d.git@v0.7.5"
```

### Data Issues

**Problem**: `FileNotFoundError: new_cam.pkl not found`
```bash
# Solution: Run preprocessing first
python scripts/preprocess.py dataset=custom mode=single_view_preprocess

# Or use monocular fitting (no preprocessing needed)
python fit_monocular.py --input_dir frames/
```

**Problem**: Poor keypoint quality
```bash
# Solution 1: Use YOLO instead of geometric
python fit_monocular.py --detector yolo

# Solution 2: Train custom detector (see Usage Scenario 3)
# Solution 3: Manually inspect and fix
python preprocessing_utils/visualize_yolo_labels.py --images frames/ --labels labels/
```

**Problem**: Camera calibration fails
```bash
# Solution: Provide known camera parameters
# Edit conf/dataset/my_dataset.yaml:
camera:
  fx: 1000.0  # Focal length X
  fy: 1000.0  # Focal length Y
  cx: 640.0   # Principal point X
  cy: 360.0   # Principal point Y
```

### Fitting Issues

**Problem**: Model converges to wrong pose
```bash
# Solution 1: Use more iterations
python fitter_articulation.py optim=accurate

# Solution 2: Start from clearer frame
python fitter_articulation.py fitter.start_frame=10

# Solution 3: Check keypoint quality
# Inspect: data/preprocessed/*/keypoints2d_undist/result_view_0.pkl
```

**Problem**: Rendering produces black images
```bash
# Solution 1: Disable rendering during debugging
python fitter_articulation.py fitter.with_render=false

# Solution 2: Check EGL libraries
ldconfig -p | grep EGL

# Solution 3: Use CPU rendering (slower)
export PYOPENGL_PLATFORM=osmesa
```

**Problem**: Very slow processing
```bash
# Solution 1: Disable rendering
python fitter_articulation.py fitter.with_render=false

# Solution 2: Use fast optimization
python fitter_articulation.py optim=fast

# Solution 3: Process fewer frames
python fitter_articulation.py fitter.end_frame=10
```

### ML Training Issues

**Problem**: YOLOv8 training fails
```bash
# Check dataset format
python << EOF
import yaml
with open('data/training/yolo_enhanced/data.yaml') as f:
    config = yaml.safe_load(f)
    print(config)
# Should contain: train, val, nc (22), names (list of 22 keypoints)
EOF

# Verify images and labels match
ls data/training/yolo_enhanced/train/images/ | wc -l
ls data/training/yolo_enhanced/train/labels/ | wc -l
# Should be equal
```

**Problem**: Low mAP after training
```bash
# Solution 1: More labeled data (add 10-20 more images)
# Solution 2: More training epochs
python scripts/train_yolo_pose.py --epochs 200

# Solution 3: Data augmentation
python scripts/train_yolo_pose.py --augment
```

---

## ðŸ“ˆ Performance Benchmarks

### Processing Time (NVIDIA RTX 3090)

| Task | Frames | Time | FPS |
|------|--------|------|-----|
| Monocular fitting (geometric) | 10 | 5 min | 0.033 |
| Monocular fitting (YOLO) | 10 | 10 min | 0.017 |
| Multi-view fitting (no render) | 10 | 25 min | 0.007 |
| Multi-view fitting (with render) | 10 | 70 min | 0.002 |
| YOLOv8 training (100 epochs) | - | 30 min | - |
| Preprocessing (OpenCV) | 100 | 1 min | 1.67 |

### Memory Usage

| Task | GPU Memory | RAM |
|------|------------|-----|
| Monocular fitting | 3-4 GB | 8 GB |
| Multi-view fitting | 4-6 GB | 16 GB |
| YOLOv8 training | 4-5 GB | 8 GB |
| Preprocessing | 2 GB | 4 GB |

### Recommendations

- **Quick testing**: Use `optim=fast`, `fitter.end_frame=5`, `fitter.with_render=false`
- **Production quality**: Use `optim=accurate`, trained YOLO detector, `fitter.with_render=true`
- **Long videos**: Process in batches of 100 frames
- **Limited GPU**: Reduce batch size, use geometric detector

---

## ðŸŽ¯ Mesh Fitting with Multiple Datasets

This project supports flexible mesh fitting across different dataset formats. See the comprehensive guide for details.

### Quick Reference

**Run with default dataset (multi-view):**
```bash
./run_mesh_fitting_default.sh 0 50     # frames 0-50
./run_mesh_fitting_default.sh 0 10 1 true  # with render
```

**Run with monocular fitting (single-view):**
```bash
./run_mesh_fitting_monocular.sh data/frames/ results/monocular/output
./run_mesh_fitting_monocular.sh data/frames/ results/monocular/output --keypoints none  # silhouette only
```

**Quick test (3 frames):**
```bash
./run_mesh_fitting_default.sh 0 3      # Multi-view test
python fit_monocular.py --input_dir data/test/ --output_dir results/test/ --max_images 3
```

### Supported Dataset Types

| Dataset | Location | Has Masks | Has Keypoints | Best Script |
|---------|----------|-----------|---------------|-------------|
| **Default Markerless** | `data/examples/markerless_mouse_1_nerf/` | âœ… | âœ… | `fitter_articulation.py` |
| **Single Images** | Any RGB+mask folder | âœ… | Optional | `fit_monocular.py` |
| **Cropped Frames** | `data/.../cropped/` | âœ… | Optional | `fit_monocular.py --keypoints none` |
| **Custom** | User-defined | Varies | Varies | Configurable |

### Configuration System

The project uses Hydra for hierarchical configuration. Available dataset configs:

- `default_markerless` - Reference multi-view dataset with 6 cameras
- `cropped` - Cropped frames with masks (single-view)
- `upsampled` - Upsampled frames (requires mask generation)
- `shank3` - Shank3 experiment dataset
- `custom` - Template for your custom data

**Override configuration from command line:**
```bash
python fitter_articulation.py \
  dataset=cropped \
  data.data_dir=/path/to/data \
  fitter.start_frame=0 \
  fitter.end_frame=100 \
  fitter.with_render=true
```

### Output Structure

```
results/fitting/{dataset}_{timestamp}/
â”œâ”€â”€ obj/
â”‚   â”œâ”€â”€ mesh_000000.obj           # 3D mesh per frame
â”‚   â””â”€â”€ ...
â”œâ”€â”€ params/
â”‚   â”œâ”€â”€ param0.pkl                # Fitted parameters
â”‚   â”œâ”€â”€ param0_sil.pkl            # After silhouette refinement
â”‚   â””â”€â”€ ...
â”œâ”€â”€ render/                       # (if with_render=true)
â”‚   â”œâ”€â”€ fitting_0.png             # Visualization overlay
â”‚   â””â”€â”€ debug/                    # Optimization debug images
â””â”€â”€ .hydra/
    â””â”€â”€ config.yaml               # Configuration used
```

**Hydra logs** are stored in: `results/logs/YYYY-MM-DD/HH-MM-SS/`

### Documentation

- **[Mesh Fitting Guide](docs/MESH_FITTING_GUIDE.md)** - Complete workflow and troubleshooting
- **[Quick Cheatsheet](MESH_FITTING_CHEATSHEET.md)** - Command reference

---

## ðŸ“š Documentation

### Complete Guides
- **[Mesh Fitting Guide](docs/MESH_FITTING_GUIDE.md)** - Multi-dataset mesh fitting workflows
- **[Monocular Fitting Guide](docs/guides/MONOCULAR_FITTING_GUIDE.md)** - Detailed single-view workflow
- **[Comprehensive Usage Guide](docs/guides/COMPREHENSIVE_USAGE_GUIDE.md)** - All usage scenarios
- **[Roboflow Labeling Guide](docs/ROBOFLOW_LABELING_GUIDE.md)** - Manual labeling tutorial
- **[SAM Mask Acquisition](docs/guides/SAM_MASK_ACQUISITION_MANUAL.md)** - High-quality masks

### Quick Reference
- **[Mesh Fitting Cheatsheet](MESH_FITTING_CHEATSHEET.md)** - Command quick reference

### Technical Reports
- **[ML Keypoint Detection](docs/reports/251115_comprehensive_ml_keypoint_summary.md)** - Complete ML workflow
- **[Implementation Summary](docs/reports/251114_ml_keypoint_detection_integration.md)** - Technical details
- **[All Reports](docs/reports/)** - Research session summaries

---

## ðŸŽ“ Key Concepts

### Three-Step Optimization

The fitting uses a progressive optimization strategy:

1. **Step 0: Global Initialization** (10 iters)
   - Objective: Find initial 3D pose
   - Uses: 2D keypoint reprojection only
   - Fast and robust to initialization

2. **Step 1: Joint Optimization** (100 iters)
   - Objective: Refine pose with all views
   - Uses: 2D keypoints + temporal smoothness
   - Main fitting stage

3. **Step 2: Silhouette Refinement** (30 iters)
   - Objective: Fine-tune surface details
   - Uses: Silhouette masks + PyTorch3D rendering
   - Highest quality but slower

### Keypoint Detectors

**Geometric** (Baseline):
- Extracts keypoints from silhouette contours
- Fast but low accuracy (~50% confidence)
- Good for: Quick testing, clean backgrounds

**YOLOv8-Pose** (Recommended):
- Pretrained on COCO, fine-tunable on your data
- Medium accuracy (~70-80% with fine-tuning)
- Good for: Most use cases

**SuperAnimal-TopViewMouse** (Future):
- Pretrained on 5K+ mice, highest accuracy
- Currently limited by API constraints
- Good for: Research applications when available

### Camera Models

**Single-View (Monocular)**:
- Estimates intrinsics from first frame
- Assumes: Known mouse size (~3cm body length)
- Limitations: Scale ambiguity, depth uncertainty

**Multi-View**:
- Uses calibrated camera parameters
- Assumes: Synchronized cameras, known calibration
- Advantages: Full 3D reconstruction, no scale ambiguity

---

## ðŸ†• Recent Updates

### 2025-11-26: CLI ì¼ê´€ì„± ê°œì„ 
- âœ… `fitter_articulation.py`ì— argparse ìŠ¤íƒ€ì¼ CLI í˜¸í™˜ì„± ì¶”ê°€
- âœ… `--keypoints none`, `--input_dir`, `--output_dir` ë“± fit_monocular.pyì™€ ë™ì¼í•œ ì¸í„°íŽ˜ì´ìŠ¤
- âœ… `fitter.use_keypoints` ì„¤ì • ì˜µì…˜ ì¶”ê°€ (keypoint loss ë¹„í™œì„±í™”)
- âœ… `fit_cropped_frames.py` deprecatedë¡œ ì´ë™ (fit_monocular.pyë¡œ í†µí•©)
- âœ… README ì—…ë°ì´íŠ¸: CLI ë§¤í•‘ í…Œì´ë¸”, ì‚¬ìš©ë²• í†µì¼

### 2025-11-25: Folder Organization and Monocular Pipeline
- âœ… Consolidated result folders to unified `results/` structure
- âœ… Added monocular fitting shell script (`run_mesh_fitting_monocular.sh`)
- âœ… Created monocular config (`conf/monocular.yaml`)
- âœ… Enhanced visualization with keypoint overlay
- âœ… Added keypoint selection by groups (head, spine, limbs, tail)
- âœ… Cleaned up git-tracked large files (2.4GB â†’ 6.5MB)
- âœ… Updated all output paths in codebase

### 2025-11-15: Major Cleanup and Documentation
- âœ… Reorganized project structure (36 â†’ 21 root items)
- âœ… Created comprehensive README with step-by-step examples
- âœ… Moved all scripts to `scripts/` directory
- âœ… Cleaned 410MB of archived outputs
- âœ… Updated all documentation paths

### 2025-11-14: ML Integration
- âœ… Monocular fitting pipeline (`fit_monocular.py`)
- âœ… YOLOv8-Pose integration
- âœ… SuperAnimal-TopViewMouse support
- âœ… Manual labeling workflow

### 2025-11-03: Preprocessing Improvements
- âœ… OpenCV-based preprocessing
- âœ… Geometric keypoint estimation
- âœ… SAM mask acquisition (experimental)

---

## ðŸ“Š Comparison with DANNCE

![comparison](assets/figs/mouse_2.png)

Results comparing DANNCE-T (temporal version) with MAMMAL_mouse on `markerless_mouse_1` sequence.

**MAMMAL_mouse advantages**:
- Full 3D mesh reconstruction (not just keypoints)
- Articulated model enforces anatomical constraints
- Compatible with single-view videos

**DANNCE advantages**:
- Faster processing
- Simpler setup (no model fitting)
- More robust to occlusions

---

## ðŸ“ Project Structure

```
MAMMAL_mouse/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚
â”œâ”€â”€ # Core Python Files
â”œâ”€â”€ fitter_articulation.py         # Main multi-view mesh fitter (Hydra + argparse ì§€ì›)
â”œâ”€â”€ fit_monocular.py               # Single-view monocular fitting (argparse)
â”œâ”€â”€ articulation_th.py             # Articulation model (PyTorch)
â”œâ”€â”€ bodymodel_th.py                # Body model (PyTorch)
â”œâ”€â”€ bodymodel_np.py                # Body model (NumPy)
â”œâ”€â”€ mouse_22_defs.py               # 22 keypoint definitions
â”œâ”€â”€ utils.py                       # Utility functions
â”‚
â”œâ”€â”€ # Shell Scripts (Quick Start)
â”œâ”€â”€ run_mesh_fitting_default.sh    # Multi-view fitting
â”œâ”€â”€ run_mesh_fitting_monocular.sh  # Monocular fitting
â”œâ”€â”€ run_unified_annotator.sh       # Launch annotation tool
â”‚
â”œâ”€â”€ # Configuration
â”œâ”€â”€ conf/                          # Hydra configs
â”‚   â”œâ”€â”€ config.yaml                # Main config
â”‚   â”œâ”€â”€ monocular.yaml             # Monocular fitting config
â”‚   â””â”€â”€ dataset/                   # Dataset-specific configs
â”‚       â”œâ”€â”€ default_markerless.yaml
â”‚       â”œâ”€â”€ cropped.yaml
â”‚       â””â”€â”€ custom.yaml
â”‚
â”œâ”€â”€ # Scripts (Organized)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ annotators/                # Annotation tools
â”‚   â”‚   â”œâ”€â”€ unified_annotator.py   # Mask + Keypoint tool (Gradio)
â”‚   â”‚   â””â”€â”€ keypoint_annotator_v2.py
â”‚   â”œâ”€â”€ preprocessing/             # Video preprocessing
â”‚   â”‚   â””â”€â”€ extract_video_frames.py
â”‚   â”œâ”€â”€ setup/                     # Installation scripts
â”‚   â”‚   â”œâ”€â”€ setup.sh
â”‚   â”‚   â”œâ”€â”€ download_superanimal.py
â”‚   â”‚   â””â”€â”€ sample_images_for_labeling.py
â”‚   â”œâ”€â”€ utils/                     # Utility scripts
â”‚   â”‚   â”œâ”€â”€ convert_keypoints_to_mammal.py
â”‚   â”‚   â””â”€â”€ process_video_with_sam.py
â”‚   â”œâ”€â”€ tests/                     # Test scripts
â”‚   â”œâ”€â”€ deprecated/                # Old/replaced scripts
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ train_yolo_pose.py
â”‚
â”œâ”€â”€ # Preprocessing Utilities
â”œâ”€â”€ preprocessing_utils/
â”‚   â”œâ”€â”€ keypoint_estimation.py     # Geometric keypoint detector
â”‚   â”œâ”€â”€ yolo_keypoint_detector.py  # YOLO-Pose detector
â”‚   â”œâ”€â”€ superanimal_detector.py    # SuperAnimal detector
â”‚   â”œâ”€â”€ mask_processing.py         # Mask utilities
â”‚   â”œâ”€â”€ sam_inference.py           # SAM integration
â”‚   â””â”€â”€ silhouette_renderer.py     # PyTorch3D rendering
â”‚
â”œâ”€â”€ # Assets (tracked)
â”œâ”€â”€ mouse_model/                   # MAMMAL parametric model
â”‚   â”œâ”€â”€ mouse.pkl                  # Main model file
â”‚   â””â”€â”€ mouse_txt/                 # Auxiliary files
â”‚
â”œâ”€â”€ # Documentation
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ guides/                    # Usage guides
â”‚   â””â”€â”€ reports/                   # Research notes (YYMMDD_*.md)
â”‚
â”œâ”€â”€ # Models (git-ignored, download separately)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ README.md                  # Download instructions
â”‚   â”œâ”€â”€ pretrained/                # SAM, YOLO base models
â”‚   â””â”€â”€ trained/                   # Fine-tuned models
â”‚
â”œâ”€â”€ # Data (git-ignored)
â”œâ”€â”€ data/                          # Input datasets
â”‚
â””â”€â”€ # Results (git-ignored)
â””â”€â”€ results/
    â”œâ”€â”€ fitting/                   # Mesh fitting outputs
    â”œâ”€â”€ monocular/                 # Monocular fitting outputs
    â””â”€â”€ logs/                      # Hydra logs
```

---

## ðŸ“§ Support

### Getting Help

1. **Check documentation**:
   - This README
   - `docs/guides/` for detailed tutorials
   - `docs/reports/` for technical details

2. **Common issues**:
   - See Troubleshooting section above
   - Check existing GitHub issues

3. **Report bugs**:
   - Open GitHub issue with:
     - Error message and full traceback
     - Your environment: `conda list > environment.txt`
     - Config file used
     - Minimal reproducible example

### Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Add tests if applicable
4. Update documentation
5. Submit pull request

---

## ðŸ™ Acknowledgments

- **MAMMAL framework**: An et al. (2023)
- **Virtual mouse model**: Bolanos et al. (2021)
- **DANNCE dataset**: Dunn et al. (2021)
- **PyTorch3D**: Meta AI Research
- **YOLOv8**: Ultralytics
- **SuperAnimal**: Mathis Lab

---

## ðŸ§ª Ablation Study Experiments

ì²´ê³„ì ì¸ ablation studyë¥¼ ìœ„í•œ ì‹¤í—˜ ê°€ì´ë“œìž…ë‹ˆë‹¤.

### ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©ë²•

```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í—˜ ëª©ë¡ ë³´ê¸°
./run_experiment.sh

# ë””ë²„ê·¸ ëª¨ë“œ (2 frames, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
./run_experiment.sh <experiment_name> --debug

# ì „ì²´ ì‹¤í–‰
./run_experiment.sh <experiment_name>

# ì»¤ìŠ¤í…€ í”„ë ˆìž„ ìˆ˜
./run_experiment.sh <experiment_name> --frames 50
```

### ì‹¤í—˜ ê·¸ë£¹

#### Group 1: Baseline (Paper Reference)

| Experiment | Views | Keypoints | Description |
|------------|-------|-----------|-------------|
| `baseline_6view_keypoint` | 6 | 22 (full) | MAMMAL ë…¼ë¬¸ ê¸°ë³¸ ì„¤ì • |

```bash
./run_experiment.sh baseline_6view_keypoint --debug   # í…ŒìŠ¤íŠ¸
./run_experiment.sh baseline_6view_keypoint           # ì „ì²´ ì‹¤í–‰
```

#### Group 2: Keypoint Ablation (6-view ê³ ì •)

| Experiment | Views | Keypoints | Description |
|------------|-------|-----------|-------------|
| `baseline_6view_keypoint` | 6 | 22 | Full keypoints |
| `sixview_sparse_keypoint` | 6 | 3 | Sparse (nose, neck, tail) |
| `sixview_no_keypoint` | 6 | 0 | Silhouette only |

```bash
# ì „ì²´ ê·¸ë£¹ ì‹¤í–‰
for exp in baseline_6view_keypoint sixview_sparse_keypoint sixview_no_keypoint; do
    ./run_experiment.sh $exp --debug
done
```

#### Group 3: Viewpoint Ablation (Sparse 3 keypoints ê³ ì •)

| Experiment | Views | Cameras | Description |
|------------|-------|---------|-------------|
| `sixview_sparse_keypoint` | 6 | 0,1,2,3,4,5 | Reference |
| `sparse_5view` | 5 | 0,1,2,3,4 | Drop camera 5 |
| `sparse_4view` | 4 | 0,1,2,3 | 4 consecutive |
| `sparse_3view` | 3 | 0,2,4 | Diagonal (better coverage) |
| `sparse_2view` | 2 | 0,3 | Opposite (stereo-like) |

```bash
# ì „ì²´ ê·¸ë£¹ ì‹¤í–‰
for exp in sixview_sparse_keypoint sparse_5view sparse_4view sparse_3view sparse_2view; do
    ./run_experiment.sh $exp --debug
done
```

### Sparse Keypoint ì„¤ì •

**ì¤‘ìš”**: GT annotationê³¼ Model definitionì˜ Head keypoint ìˆœì„œê°€ ë‹¤ë¦…ë‹ˆë‹¤!

| Index | GT (mouse_22_defs.py) | Model (keypoint22_mapper.json) |
|-------|----------------------|-------------------------------|
| 0 | left_ear | nose |
| 1 | right_ear | left_ear |
| 2 | **nose** | right_ear |
| 3+ | ë™ì¼ | ë™ì¼ |

ì‹¤ì œ ë°ì´í„°ëŠ” GT ì •ì˜ë¥¼ ë”°ë¥´ë¯€ë¡œ sparse indicesëŠ” `[2, 5, 3]` (nose, tail_root, neck)ìž…ë‹ˆë‹¤.

ìžì„¸í•œ keypoint ì •ë³´ëŠ” `docs/KEYPOINT_REFERENCE.md` ì°¸ì¡°.

### ê²°ê³¼ ë¹„êµ

```bash
# ê²°ê³¼ ë””ë ‰í† ë¦¬ êµ¬ì¡°
results/fitting/
â”œâ”€â”€ markerless_mouse_1_nerf_v012345_kp22_*/    # Baseline
â”œâ”€â”€ markerless_mouse_1_nerf_v012345_sparse3_*/ # 6view sparse
â”œâ”€â”€ markerless_mouse_1_nerf_v012345_noKP_*/    # 6view no keypoint
â”œâ”€â”€ markerless_mouse_1_nerf_v01234_sparse3_*/  # 5view sparse
â””â”€â”€ ...

# ê²°ê³¼ ì‹œê°í™” ë¹„êµ
ls results/fitting/*/render/fitting_*.png
```

### Debug vs Full ì‹¤í–‰ ë¹„êµ

| Mode | Frames | Step0 | Step1 | Step2 | ì˜ˆìƒ ì‹œê°„ |
|------|--------|-------|-------|-------|----------|
| Debug | 2 | 5 | 20 | 10 | ~1ë¶„ |
| Full | 100 | 10-20 | 100-180 | 30-50 | ~30ë¶„ |

---

## ðŸ“„ License

[Specify your license here]

---

## ðŸ“š Citation

If you use this code, please cite:

```bibtex
@article{MAMMAL,
    author = {An, Liang and Ren, Jilong and Yu, Tao and Hai, Tang and Jia, Yichang and Liu, Yebin},
    title = {Three-dimensional surface motion capture of multiple freely moving pigs using MAMMAL},
    journal = {},
    year = {2023}
}

@article{bolanos2021three,
  title={A three-dimensional virtual mouse generates synthetic training data for behavioral analysis},
  author={Bola{\~n}os, Luis A and Xiao, Dongsheng and Ford, Nancy L and LeDue, Jeff M and Gupta, Pankaj K and Doebeli, Carlos and Hu, Hao and Rhodin, Helge and Murphy, Timothy H},
  journal={Nature methods},
  volume={18},
  number={4},
  pages={378--381},
  year={2021},
  publisher={Nature Publishing Group US New York}
}
```
